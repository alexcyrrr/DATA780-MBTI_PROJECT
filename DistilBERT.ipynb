{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This uses the kaggle_data.csv file and a combined reddit_full.csv file. Ran on Google Colab T4 GPU for efficiency purposes.\n",
        "\n",
        "Implemented by Amanda Pignataro.\n",
        "\n",
        "Resources:\n",
        "\n",
        "*   https://www.geeksforgeeks.org/nlp/distilbert-in-natural-language-processing/\n",
        "*   https://medium.com/thenextlayer/building-a-text-classification-model-using-distilbert-703c1409696c\n",
        "*   https://lewtun.github.io/transformerlab/experiments.distilbert.html\n",
        "*   For debugging/Second part issues with TF and Keras: ChatGPT\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c7k2hZcoVqRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST PART:** DistilBERT Implementation with 16-class Model"
      ],
      "metadata": {
        "id": "4BX6DibcWVkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers==4.44.2\""
      ],
      "metadata": {
        "id": "vScFNwpiVJQy",
        "outputId": "7e281a2a-f954-40b8-f3ac-a38d916f7f6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.12/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.6.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ],
      "metadata": {
        "id": "EkCtNzP4G2au"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQxVcAYAPdmj",
        "outputId": "22f485b2-7b67-4ab3-eed5-fa84c2e495cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.19.0\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Loading Kaggle + Reddit CSVs...\n",
            "For BERT we will use:\n",
            "  Kaggle: 50000 rows\n",
            "  Reddit: 100000 rows\n"
          ]
        }
      ],
      "source": [
        "##This will handle imports, GPU, label setup, load CSVs\n",
        "\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertModel\n",
        "\n",
        "##This will set up TF and GPU\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "for g in gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"GPUs:\", gpus)\n",
        "\n",
        "##This will bring our files in\n",
        "KAGGLE_CSV = \"kaggle_data.csv\"\n",
        "REDDIT_CSV = \"reddit_full.csv\"\n",
        "\n",
        "##These are our MBTI labels\n",
        "MBTI16 = [\n",
        "    \"ISTJ\",\"ISFJ\",\"INFJ\",\"INTJ\",\n",
        "    \"ISTP\",\"ISFP\",\"INFP\",\"INTP\",\n",
        "    \"ENTJ\",\"ENTP\",\"ENFJ\",\"ENFP\",\n",
        "    \"ESTJ\",\"ESFJ\",\"ESTP\",\"ESFP\"\n",
        "]\n",
        "lab2id = {l:i for i,l in enumerate(MBTI16)}\n",
        "\n",
        "##This will help clean text and load CSV files\n",
        "def liststr_to_str(x):\n",
        "    \"\"\"\n",
        "    This will determine if x looks like \"['post1','post2']\", turn into a single string.\n",
        "    Otherwise just str(x).\n",
        "    \"\"\"\n",
        "    if isinstance(x, str) and x.startswith('['):\n",
        "        try:\n",
        "            toks = ast.literal_eval(x)\n",
        "            if isinstance(toks, list):\n",
        "                return \" \".join(map(str, toks))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return str(x)\n",
        "\n",
        "def load_df(path, text_col_guess=(\"posts\",\"body\",\"text\"),\n",
        "            label_col_guess=(\"type\",\"class\",\"label\")):\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    text_col  = next((c for c in text_col_guess  if c in df.columns), None)\n",
        "    label_col = next((c for c in label_col_guess if c in df.columns), None)\n",
        "    assert text_col and label_col, (\n",
        "        f\"Could not find text/label in {path}. \"\n",
        "        f\"Columns: {df.columns.tolist()}\"\n",
        "    )\n",
        "\n",
        "    df = df[[text_col, label_col]].rename(columns={\n",
        "        text_col: \"text\",\n",
        "        label_col: \"label\"\n",
        "    })\n",
        "    df[\"text\"]  = df[\"text\"].map(liststr_to_str)\n",
        "    df[\"label\"] = df[\"label\"].astype(str)\n",
        "\n",
        "    df = df[df[\"label\"].isin(MBTI16)].copy()\n",
        "    df[\"y\"] = df[\"label\"].map(lab2id).astype(\"int32\")\n",
        "    return df\n",
        "\n",
        "print(\"Loading Kaggle + Reddit CSVs...\")\n",
        "df_k = load_df(KAGGLE_CSV, text_col_guess=(\"posts\",\"body\"), label_col_guess=(\"type\",\"class\"))\n",
        "df_r = load_df(REDDIT_CSV, text_col_guess=(\"body\",\"posts\"), label_col_guess=(\"class\",\"type\"))\n",
        "\n",
        "##This will make smaller subsets for DistilBERT\n",
        "\n",
        "##This will set how many examples per domain to use\n",
        "N_K_BERT = min(len(df_k), 50000)\n",
        "N_R_BERT = min(len(df_r), 100000)\n",
        "\n",
        "##This will sample without replacement for robustness\n",
        "df_k_bert = df_k.sample(N_K_BERT, random_state=42).reset_index(drop=True)\n",
        "df_r_bert = df_r.sample(N_R_BERT, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"For BERT we will use:\")\n",
        "print(\"  Kaggle:\", len(df_k_bert), \"rows\")\n",
        "print(\"  Reddit:\", len(df_r_bert), \"rows\")\n",
        "\n",
        "yk_bert = df_k_bert[\"y\"].to_numpy().astype(\"int32\")\n",
        "yr_bert = df_r_bert[\"y\"].to_numpy().astype(\"int32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will set the DistilBERT tokenizer + encode texts on subsets\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN    = 128\n",
        "\n",
        "print(\"Loading DistilBERT tokenizer:\", MODEL_NAME)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def encode_texts(texts, max_len=MAX_LEN):\n",
        "    \"\"\"\n",
        "    This will encode a Series/list of texts into input_ids and attention_mask arrays.\n",
        "    \"\"\"\n",
        "    enc = tokenizer(\n",
        "        list(texts.astype(str)),\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    return enc[\"input_ids\"], enc[\"attention_mask\"]\n",
        "\n",
        "print(\"Encoding Kaggle texts (subset)...\")\n",
        "Xk_ids, Xk_mask = encode_texts(df_k_bert[\"text\"])\n",
        "\n",
        "print(\"Encoding Reddit texts (subset)...\")\n",
        "Xr_ids, Xr_mask = encode_texts(df_r_bert[\"text\"])\n",
        "\n",
        "print(\"Kaggle encodings:\", Xk_ids.shape, Xk_mask.shape)\n",
        "print(\"Reddit encodings:\", Xr_ids.shape, Xr_mask.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX-yF-RmT1R_",
        "outputId": "62671c41-48d3-4264-941d-23b7415f2bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DistilBERT tokenizer: distilbert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding Kaggle texts (subset)...\n",
            "Encoding Reddit texts (subset)...\n",
            "Kaggle encodings: (50000, 128) (50000, 128)\n",
            "Reddit encodings: (100000, 128) (100000, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will Train/val splits + tf.data datasets\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH    = 16\n",
        "\n",
        "def make_bert_split(input_ids, attention_mask, y, frac=0.9, shuffle=True):\n",
        "    n = len(y)\n",
        "    k = int(n * frac)\n",
        "\n",
        "    ids_tr, ids_va   = input_ids[:k],      input_ids[k:]\n",
        "    mask_tr, mask_va = attention_mask[:k], attention_mask[k:]\n",
        "    y_tr, y_va       = y[:k],              y[k:]\n",
        "\n",
        "    if shuffle:\n",
        "        idx = np.random.permutation(len(y_tr))\n",
        "        ids_tr  = ids_tr[idx]\n",
        "        mask_tr = mask_tr[idx]\n",
        "        y_tr    = y_tr[idx]\n",
        "\n",
        "    ds_tr = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices(\n",
        "              ({\"input_ids\": ids_tr, \"attention_mask\": mask_tr}, y_tr)\n",
        "          )\n",
        "          .shuffle(10000)\n",
        "          .batch(BATCH)\n",
        "          .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    ds_va = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices(\n",
        "              ({\"input_ids\": ids_va, \"attention_mask\": mask_va}, y_va)\n",
        "          )\n",
        "          .batch(BATCH)\n",
        "          .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    return ds_tr, ds_va, (ids_va, mask_va, y_va)\n",
        "\n",
        "print(\"Building train/val splits...\")\n",
        "ds_k_tr_bert, ds_k_va_bert, (Xk_ids_va, Xk_mask_va, yk_va) = make_bert_split(Xk_ids, Xk_mask, yk_bert)\n",
        "ds_r_tr_bert, ds_r_va_bert, (Xr_ids_va, Xr_mask_va, yr_va) = make_bert_split(Xr_ids, Xr_mask, yr_bert)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyLC5iDJUB1_",
        "outputId": "9c025e85-6e9a-48df-99c4-92f5e04c19e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building train/val splits...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will establish the DistilBERT classifier model\n",
        "\n",
        "def make_distilbert_classifier(num_labels=16, lr=3e-5):\n",
        "    bert = TFDistilBertModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    input_ids      = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    outputs = bert(input_ids, attention_mask=attention_mask)\n",
        "    cls_tok = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    h = tf.keras.layers.Dropout(0.2)(cls_tok)\n",
        "    logits = tf.keras.layers.Dense(num_labels, activation=\"softmax\", name=\"type\")(h)\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
        "        outputs=logits\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "cb_bert = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=1,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "9BPSYmI3Rq2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This will train DistilBERT on Kaggle + Reddit\n",
        "\n",
        "print(\"\\n=== Train DistilBERT on Kaggle ===\")\n",
        "m_k_bert = make_distilbert_classifier(num_labels=16, lr=3e-5)\n",
        "hist_k_bert = m_k_bert.fit(\n",
        "    ds_k_tr_bert,\n",
        "    validation_data=ds_k_va_bert,\n",
        "    epochs=2,\n",
        "    callbacks=cb_bert,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n=== Train DistilBERT on Reddit ===\")\n",
        "m_r_bert = make_distilbert_classifier(num_labels=16, lr=3e-5)\n",
        "hist_r_bert = m_r_bert.fit(\n",
        "    ds_r_tr_bert,\n",
        "    validation_data=ds_r_va_bert,\n",
        "    epochs=2,\n",
        "    callbacks=cb_bert,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlI4gdkPRtLk",
        "outputId": "22d79c5f-8451-449b-db0f-35773e42d13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Train DistilBERT on Kaggle ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2813/2813 [==============================] - 629s 214ms/step - loss: 2.2820 - accuracy: 0.2059 - val_loss: 2.2505 - val_accuracy: 0.2234\n",
            "Epoch 2/2\n",
            "2813/2813 [==============================] - 596s 212ms/step - loss: 2.1936 - accuracy: 0.2483 - val_loss: 2.2653 - val_accuracy: 0.2222\n",
            "\n",
            "=== Train DistilBERT on Reddit ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "5625/5625 [==============================] - 1257s 219ms/step - loss: 2.0156 - accuracy: 0.2734 - val_loss: 1.9726 - val_accuracy: 0.2959\n",
            "Epoch 2/2\n",
            "5625/5625 [==============================] - 1194s 212ms/step - loss: 1.9179 - accuracy: 0.3186 - val_loss: 1.9747 - val_accuracy: 0.2998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will set up our evaluation helper function\n",
        "\n",
        "def eval_bert_model(model, ids, mask, y, tag, batch_size=64):\n",
        "    ds = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices({\"input_ids\": ids, \"attention_mask\": mask})\n",
        "          .batch(batch_size)\n",
        "    )\n",
        "    yhat = model.predict(ds, verbose=0)\n",
        "    pred = np.argmax(yhat, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y, pred)\n",
        "    f1  = f1_score(y, pred, average=\"macro\")\n",
        "\n",
        "    print(f\"\\n--- {tag} ---\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Macro-F1:\", f1)\n",
        "    print(classification_report(y, pred, target_names=MBTI16, digits=3))\n",
        "\n",
        "    return acc, f1\n"
      ],
      "metadata": {
        "id": "MDWODJAgRvWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This will show the four experiments and ΔF1 summary\n",
        "\n",
        "##1) Kaggle → Kaggle (in-domain)\n",
        "acc_k_in_bert, f1_k_in_bert = eval_bert_model(\n",
        "    m_k_bert,\n",
        "    Xk_ids_va, Xk_mask_va, yk_va,\n",
        "    \"DistilBERT Kaggle → Kaggle (val)\"\n",
        ")\n",
        "\n",
        "##2) Reddit → Reddit (in-domain)\n",
        "acc_r_in_bert, f1_r_in_bert = eval_bert_model(\n",
        "    m_r_bert,\n",
        "    Xr_ids_va, Xr_mask_va, yr_va,\n",
        "    \"DistilBERT Reddit → Reddit (val)\"\n",
        ")\n",
        "\n",
        "##3) Kaggle → Reddit (cross)\n",
        "acc_k2r_bert, f1_k2r_bert = eval_bert_model(\n",
        "    m_k_bert,\n",
        "    Xr_ids_va, Xr_mask_va, yr_va,\n",
        "    \"DistilBERT Kaggle → Reddit (cross)\"\n",
        ")\n",
        "\n",
        "##4) Reddit → Kaggle (cross)\n",
        "acc_r2k_bert, f1_r2k_bert = eval_bert_model(\n",
        "    m_r_bert,\n",
        "    Xk_ids_va, Xk_mask_va, yk_va,\n",
        "    \"DistilBERT Reddit → Kaggle (cross)\"\n",
        ")\n",
        "\n",
        "##ΔF1 summary\n",
        "print(\"\\n=== DistilBERT cross-domain drop (ΔF1 = within − cross) ===\")\n",
        "print(f\"Kaggle-trained ΔF1: {f1_k_in_bert - f1_k2r_bert:.4f}\")\n",
        "print(f\"Reddit-trained ΔF1: {f1_r_in_bert - f1_r2k_bert:.4f}\")\n"
      ],
      "metadata": {
        "id": "IAFu2awVRyRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc34ffd0-f776-4e40-c32d-821efa802015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DistilBERT Kaggle → Kaggle (val) ---\n",
            "Accuracy: 0.2234\n",
            "Macro-F1: 0.05316244062945563\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.000     0.000     0.000       136\n",
            "        ISFJ      0.000     0.000     0.000        91\n",
            "        INFJ      0.227     0.393     0.288       876\n",
            "        INTJ      0.196     0.016     0.029       637\n",
            "        ISTP      0.000     0.000     0.000       196\n",
            "        ISFP      0.000     0.000     0.000       173\n",
            "        INFP      0.216     0.629     0.322      1006\n",
            "        INTP      0.258     0.172     0.207       749\n",
            "        ENTJ      0.000     0.000     0.000       139\n",
            "        ENTP      0.000     0.000     0.000       396\n",
            "        ENFJ      0.000     0.000     0.000       119\n",
            "        ENFP      0.143     0.003     0.005       374\n",
            "        ESTJ      0.000     0.000     0.000        27\n",
            "        ESFJ      0.000     0.000     0.000        13\n",
            "        ESTP      0.000     0.000     0.000        46\n",
            "        ESFP      0.000     0.000     0.000        22\n",
            "\n",
            "    accuracy                          0.223      5000\n",
            "   macro avg      0.065     0.076     0.053      5000\n",
            "weighted avg      0.158     0.223     0.150      5000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DistilBERT Reddit → Reddit (val) ---\n",
            "Accuracy: 0.2959\n",
            "Macro-F1: 0.07838966341325508\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.000     0.000     0.000        98\n",
            "        ISFJ      0.000     0.000     0.000        29\n",
            "        INFJ      0.266     0.165     0.204      1218\n",
            "        INTJ      0.267     0.285     0.276      2132\n",
            "        ISTP      0.000     0.000     0.000       303\n",
            "        ISFP      0.000     0.000     0.000        70\n",
            "        INFP      0.245     0.099     0.141      1044\n",
            "        INTP      0.325     0.685     0.441      2793\n",
            "        ENTJ      0.000     0.000     0.000       271\n",
            "        ENTP      0.208     0.097     0.132      1158\n",
            "        ENFJ      0.000     0.000     0.000       120\n",
            "        ENFP      0.182     0.037     0.061       596\n",
            "        ESTJ      0.000     0.000     0.000        28\n",
            "        ESFJ      0.000     0.000     0.000        25\n",
            "        ESTP      0.000     0.000     0.000        79\n",
            "        ESFP      0.000     0.000     0.000        36\n",
            "\n",
            "    accuracy                          0.296     10000\n",
            "   macro avg      0.093     0.085     0.078     10000\n",
            "weighted avg      0.241     0.296     0.240     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DistilBERT Kaggle → Reddit (cross) ---\n",
            "Accuracy: 0.1642\n",
            "Macro-F1: 0.04295950338946858\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.000     0.000     0.000        98\n",
            "        ISFJ      0.000     0.000     0.000        29\n",
            "        INFJ      0.178     0.385     0.243      1218\n",
            "        INTJ      0.220     0.008     0.016      2132\n",
            "        ISTP      0.000     0.000     0.000       303\n",
            "        ISFP      0.000     0.000     0.000        70\n",
            "        INFP      0.112     0.640     0.191      1044\n",
            "        INTP      0.371     0.174     0.237      2793\n",
            "        ENTJ      0.000     0.000     0.000       271\n",
            "        ENTP      0.000     0.000     0.000      1158\n",
            "        ENFJ      0.000     0.000     0.000       120\n",
            "        ENFP      0.000     0.000     0.000       596\n",
            "        ESTJ      0.000     0.000     0.000        28\n",
            "        ESFJ      0.000     0.000     0.000        25\n",
            "        ESTP      0.000     0.000     0.000        79\n",
            "        ESFP      0.000     0.000     0.000        36\n",
            "\n",
            "    accuracy                          0.164     10000\n",
            "   macro avg      0.055     0.075     0.043     10000\n",
            "weighted avg      0.184     0.164     0.119     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DistilBERT Reddit → Kaggle (cross) ---\n",
            "Accuracy: 0.1744\n",
            "Macro-F1: 0.05309070414596329\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.000     0.000     0.000       136\n",
            "        ISFJ      0.000     0.000     0.000        91\n",
            "        INFJ      0.257     0.089     0.132       876\n",
            "        INTJ      0.135     0.257     0.177       637\n",
            "        ISTP      0.000     0.000     0.000       196\n",
            "        ISFP      0.000     0.000     0.000       173\n",
            "        INFP      0.321     0.087     0.138      1006\n",
            "        INTP      0.176     0.677     0.279       749\n",
            "        ENTJ      0.000     0.000     0.000       139\n",
            "        ENTP      0.091     0.061     0.073       396\n",
            "        ENFJ      0.000     0.000     0.000       119\n",
            "        ENFP      0.180     0.029     0.051       374\n",
            "        ESTJ      0.000     0.000     0.000        27\n",
            "        ESFJ      0.000     0.000     0.000        13\n",
            "        ESTP      0.000     0.000     0.000        46\n",
            "        ESFP      0.000     0.000     0.000        22\n",
            "\n",
            "    accuracy                          0.174      5000\n",
            "   macro avg      0.073     0.075     0.053      5000\n",
            "weighted avg      0.174     0.174     0.125      5000\n",
            "\n",
            "\n",
            "=== DistilBERT cross-domain drop (ΔF1 = within − cross) ===\n",
            "Kaggle-trained ΔF1: 0.0102\n",
            "Reddit-trained ΔF1: 0.0253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECOND PART:** Re-train DistilBERT with 4 binary axes (IE, SN, TF, JP)\n"
      ],
      "metadata": {
        "id": "Rtk8voGzmUn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##This will handle imports, GPU, label setup, load CSVs\n",
        "\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertModel\n",
        "\n",
        "##similar comments above\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "for g in gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"GPUs:\", gpus)\n",
        "\n",
        "##This will load our datasets\n",
        "KAGGLE_CSV = \"kaggle_data.csv\"\n",
        "REDDIT_CSV = \"reddit_full.csv\"\n",
        "\n",
        "##This will setup MBTI 16 labels (for filtering)\n",
        "MBTI16 = [\n",
        "    \"ISTJ\",\"ISFJ\",\"INFJ\",\"INTJ\",\n",
        "    \"ISTP\",\"ISFP\",\"INFP\",\"INTP\",\n",
        "    \"ENTJ\",\"ENTP\",\"ENFJ\",\"ENFP\",\n",
        "    \"ESTJ\",\"ESFJ\",\"ESTP\",\"ESFP\"\n",
        "]\n",
        "\n",
        "AXES = [\"IE\", \"NS\", \"TF\", \"JP\"]\n",
        "\n",
        "##This will help to clean text and derive 4 axes\n",
        "def liststr_to_str(x):\n",
        "    \"\"\"\n",
        "    This will determine if x looks like \"['post1','post2']\", turn into a single string.\n",
        "    Otherwise just str(x).\n",
        "    \"\"\"\n",
        "    if isinstance(x, str) and x.startswith('['):\n",
        "        try:\n",
        "            toks = ast.literal_eval(x)\n",
        "            if isinstance(toks, list):\n",
        "                return \" \".join(map(str, toks))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return str(x)\n",
        "\n",
        "def mbti_to_axes_vec(label):\n",
        "    \"\"\"\n",
        "    This will convert a 4-letter MBTI type (e.g., 'INFJ') into a 4-dim binary vector:\n",
        "\n",
        "      [IE, NS, TF, JP]\n",
        "\n",
        "    Convention (0 = first letter, 1 = second letter):\n",
        "\n",
        "      - IE: 0 = I, 1 = E\n",
        "      - NS: 0 = N, 1 = S\n",
        "      - TF: 0 = T, 1 = F\n",
        "      - JP: 0 = J, 1 = P\n",
        "    \"\"\"\n",
        "    label = str(label).upper()\n",
        "    assert label in MBTI16, f\"Unknown MBTI type: {label}\"\n",
        "\n",
        "    v_IE = 0 if label[0] == \"I\" else 1\n",
        "    v_NS = 0 if label[1] == \"N\" else 1\n",
        "    v_TF = 0 if label[2] == \"T\" else 1\n",
        "    v_JP = 0 if label[3] == \"J\" else 1\n",
        "    return np.array([v_IE, v_NS, v_TF, v_JP], dtype=\"int32\")\n",
        "\n",
        "def load_df(path, text_col_guess=(\"posts\",\"body\",\"text\"),\n",
        "            label_col_guess=(\"type\",\"class\",\"label\")):\n",
        "    \"\"\"\n",
        "    This will load CSV, find text/label columns, clean the text, keep only valid MBTI16,\n",
        "    and create 4 axis columns (ax_IE, ax_NS, ax_TF, ax_JP).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    text_col  = next((c for c in text_col_guess  if c in df.columns), None)\n",
        "    label_col = next((c for c in label_col_guess if c in df.columns), None)\n",
        "    assert text_col and label_col, (\n",
        "        f\"Could not find text/label in {path}. \"\n",
        "        f\"Columns: {df.columns.tolist()}\"\n",
        "    )\n",
        "\n",
        "    df = df[[text_col, label_col]].rename(columns={\n",
        "        text_col: \"text\",\n",
        "        label_col: \"label\"\n",
        "    })\n",
        "    df[\"text\"]  = df[\"text\"].map(liststr_to_str)\n",
        "    df[\"label\"] = df[\"label\"].astype(str).str.upper()\n",
        "\n",
        "    ##Thi swill keep only the 16 standard MBTI types\n",
        "    df = df[df[\"label\"].isin(MBTI16)].copy()\n",
        "\n",
        "    ##This will compute the 4 axes\n",
        "    axes_array = np.vstack(df[\"label\"].map(mbti_to_axes_vec).to_numpy())\n",
        "    df[\"ax_IE\"] = axes_array[:, 0]\n",
        "    df[\"ax_NS\"] = axes_array[:, 1]\n",
        "    df[\"ax_TF\"] = axes_array[:, 2]\n",
        "    df[\"ax_JP\"] = axes_array[:, 3]\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Loading Kaggle + Reddit CSVs...\")\n",
        "df_k = load_df(KAGGLE_CSV, text_col_guess=(\"posts\",\"body\"), label_col_guess=(\"type\",\"class\"))\n",
        "df_r = load_df(REDDIT_CSV, text_col_guess=(\"body\",\"posts\"), label_col_guess=(\"class\",\"type\"))\n",
        "\n",
        "print(\"Kaggle shape:\", df_k.shape)\n",
        "print(\"Reddit shape:\", df_r.shape)\n",
        "\n",
        "##This will make smaller subsets for DistilBERT\n",
        "\n",
        "##This will set how many examples per domain to use\n",
        "N_K_BERT = min(len(df_k), 50000)\n",
        "N_R_BERT = min(len(df_r), 100000)\n",
        "\n",
        "df_k_bert = df_k.sample(N_K_BERT, random_state=42).reset_index(drop=True)\n",
        "df_r_bert = df_r.sample(N_R_BERT, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"For BERT (axes) we will use:\")\n",
        "print(\"  Kaggle:\", len(df_k_bert), \"rows\")\n",
        "print(\"  Reddit:\", len(df_r_bert), \"rows\")\n",
        "\n",
        "\n",
        "yk_axes_bert = df_k_bert[[\"ax_IE\",\"ax_NS\",\"ax_TF\",\"ax_JP\"]].to_numpy().astype(\"float32\")\n",
        "yr_axes_bert = df_r_bert[[\"ax_IE\",\"ax_NS\",\"ax_TF\",\"ax_JP\"]].to_numpy().astype(\"float32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4D9oZ1NIfh1",
        "outputId": "3743f4e6-bf2c-4f04-bc45-bf59bcb86088"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.19.0\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Loading Kaggle + Reddit CSVs...\n",
            "Kaggle shape: (410915, 6)\n",
            "Reddit shape: (1651100, 6)\n",
            "For BERT (axes) we will use:\n",
            "  Kaggle: 50000 rows\n",
            "  Reddit: 100000 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will set the DistilBERT tokenizer + encode texts on subsets\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN  = 128\n",
        "\n",
        "print(\"Loading DistilBERT tokenizer:\", MODEL_NAME)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def encode_texts(texts, max_len=MAX_LEN):\n",
        "    \"\"\"\n",
        "    This will encode a Series/list of texts into input_ids and attention_mask arrays.\n",
        "    \"\"\"\n",
        "    enc = tokenizer(\n",
        "        list(texts.astype(str)),\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    return enc[\"input_ids\"], enc[\"attention_mask\"]\n",
        "\n",
        "print(\"Encoding Kaggle texts (subset)...\")\n",
        "Xk_ids, Xk_mask = encode_texts(df_k_bert[\"text\"])\n",
        "\n",
        "print(\"Encoding Reddit texts (subset)...\")\n",
        "Xr_ids, Xr_mask = encode_texts(df_r_bert[\"text\"])\n",
        "\n",
        "print(\"Kaggle encodings:\", Xk_ids.shape, Xk_mask.shape)\n",
        "print(\"Reddit encodings:\", Xr_ids.shape, Xr_mask.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asT_xxXEIgzo",
        "outputId": "78944b62-b467-4e1b-a487-6e712e2c536a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DistilBERT tokenizer: distilbert-base-uncased\n",
            "Encoding Kaggle texts (subset)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding Reddit texts (subset)...\n",
            "Kaggle encodings: (50000, 128) (50000, 128)\n",
            "Reddit encodings: (100000, 128) (100000, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will train/val splits + tf.data datasets (axes)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH    = 16\n",
        "\n",
        "def make_bert_split(input_ids, attention_mask, y_axes, frac=0.9, shuffle=True):\n",
        "    \"\"\"\n",
        "    This will split into train/val and build tf.data datasets.\n",
        "\n",
        "    y_axes is shape (N, 4) with binary labels for [IE, SN, TF, JP].\n",
        "    \"\"\"\n",
        "    n = len(y_axes)\n",
        "    k = int(n * frac)\n",
        "\n",
        "    ids_tr, ids_va   = input_ids[:k],         input_ids[k:]\n",
        "    mask_tr, mask_va = attention_mask[:k],    attention_mask[k:]\n",
        "    y_tr, y_va       = y_axes[:k],            y_axes[k:]\n",
        "\n",
        "    if shuffle:\n",
        "        idx = np.random.permutation(len(y_tr))\n",
        "        ids_tr  = ids_tr[idx]\n",
        "        mask_tr = mask_tr[idx]\n",
        "        y_tr    = y_tr[idx]\n",
        "\n",
        "    ds_tr = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices(\n",
        "              ({\"input_ids\": ids_tr, \"attention_mask\": mask_tr}, y_tr)\n",
        "          )\n",
        "          .shuffle(10000)\n",
        "          .batch(BATCH)\n",
        "          .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    ds_va = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices(\n",
        "              ({\"input_ids\": ids_va, \"attention_mask\": mask_va}, y_va)\n",
        "          )\n",
        "          .batch(BATCH)\n",
        "          .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    return ds_tr, ds_va, (ids_va, mask_va, y_va)\n",
        "\n",
        "print(\"Building train/val splits for axes...\")\n",
        "ds_k_tr_axes, ds_k_va_axes, (Xk_ids_va, Xk_mask_va, yk_axes_va) = make_bert_split(\n",
        "    Xk_ids, Xk_mask, yk_axes_bert\n",
        ")\n",
        "ds_r_tr_axes, ds_r_va_axes, (Xr_ids_va, Xr_mask_va, yr_axes_va) = make_bert_split(\n",
        "    Xr_ids, Xr_mask, yr_axes_bert\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKR02wYXIpDn",
        "outputId": "350862cd-34e3-4649-e7d5-6c64a483ccab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building train/val splits for axes...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will create the DistilBERT classifier model (4-axis, multi-label)\n",
        "\n",
        "def make_distilbert_axes_classifier(num_axes=4, lr=3e-5):\n",
        "    \"\"\"\n",
        "    This will be a DistilBERT multi-label classifier for 4 axes.\n",
        "    Outputs shape (batch, num_axes) with sigmoid activation.\n",
        "\n",
        "    Bypassing the transformers input_processing wrapper here\n",
        "    by calling the underlying DistilBERT encoder (bert_model.distilbert)\n",
        "    inside a Keras Lambda layer.\n",
        "    \"\"\"\n",
        "    bert_model = TFDistilBertModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    ##This will get hidden size from config (DistilBERT uses `dim`)\n",
        "    hidden_size = getattr(bert_model.config, \"dim\", None)\n",
        "    if hidden_size is None:\n",
        "        hidden_size = getattr(bert_model.config, \"hidden_size\")\n",
        "\n",
        "    ##This will use keras functional inputs\n",
        "    input_ids      = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    ##This will wrap the DistilBERT encoder in a Lambda to make sure it works with Keras\n",
        "    def distilbert_encoder(inputs):\n",
        "        ids, mask = inputs\n",
        "\n",
        "        ##This will call the encoder directly\n",
        "        outputs = bert_model.distilbert(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "        )\n",
        "\n",
        "\n",
        "        if isinstance(outputs, tuple):\n",
        "            hidden_states = outputs[0]\n",
        "        else:\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "    ##This will use an explicit output_shape for Lambda b/c of Keras\n",
        "    sequence_output = tf.keras.layers.Lambda(\n",
        "        distilbert_encoder,\n",
        "        name=\"distilbert_encoder\",\n",
        "        output_shape=(MAX_LEN, hidden_size),\n",
        "    )([input_ids, attention_mask])\n",
        "\n",
        "\n",
        "    cls_tok = sequence_output[:, 0, :]\n",
        "\n",
        "    h = tf.keras.layers.Dropout(0.2)(cls_tok)\n",
        "    logits = tf.keras.layers.Dense(num_axes, activation=\"sigmoid\", name=\"axes\")(h)\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
        "        outputs=logits\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"binary_crossentropy\",   # multi-label loss\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\")]\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "F0byIno2KR8M"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This will train DistilBERT axis models on Kaggle + Reddit\n",
        "\n",
        "print(\"\\n=== Train DistilBERT (4 axes) on Kaggle ===\")\n",
        "m_k_axes = make_distilbert_axes_classifier(num_axes=4, lr=3e-5)\n",
        "hist_k_axes = m_k_axes.fit(\n",
        "    ds_k_tr_axes,\n",
        "    validation_data=ds_k_va_axes,\n",
        "    epochs=2,\n",
        "    callbacks=cb_axes,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n=== Train DistilBERT (4 axes) on Reddit ===\")\n",
        "m_r_axes = make_distilbert_axes_classifier(num_axes=4, lr=3e-5)\n",
        "hist_r_axes = m_r_axes.fit(\n",
        "    ds_r_tr_axes,\n",
        "    validation_data=ds_r_va_axes,\n",
        "    epochs=2,\n",
        "    callbacks=cb_axes,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_YQGeInIuto",
        "outputId": "ed66f72e-4cac-4874-b77c-17a0e69aa71c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Train DistilBERT (4 axes) on Kaggle ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 64ms/step - binary_accuracy: 0.6284 - loss: 0.6287 - val_binary_accuracy: 0.6953 - val_loss: 0.5762\n",
            "Epoch 2/2\n",
            "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 61ms/step - binary_accuracy: 0.6923 - loss: 0.5792 - val_binary_accuracy: 0.6985 - val_loss: 0.5739\n",
            "\n",
            "=== Train DistilBERT (4 axes) on Reddit ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 62ms/step - binary_accuracy: 0.7109 - loss: 0.5543 - val_binary_accuracy: 0.7487 - val_loss: 0.5171\n",
            "Epoch 2/2\n",
            "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 61ms/step - binary_accuracy: 0.7455 - loss: 0.5212 - val_binary_accuracy: 0.7496 - val_loss: 0.5142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will print evaluation for 4 axes (IE, SN, TF, JP)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "AXIS_DISPLAY_NAMES = [\"IE\", \"SN\", \"TF\", \"JP\"]\n",
        "\n",
        "def eval_bert_axes_model(model, ids, mask, y_axes, run_name,\n",
        "                         batch_size=64, thresh=0.5):\n",
        "    \"\"\"\n",
        "    This will print per-axis metrics in the style:\n",
        "\n",
        "        --- Kaggle → Kaggle (val): Per-axis metrics ---\n",
        "        IE: acc=0.600  macroF1=0.531\n",
        "        SN: acc=0.638  macroF1=0.505\n",
        "        TF: acc=0.599  macroF1=0.597\n",
        "        JP: acc=0.541  macroF1=0.536\n",
        "\n",
        "    Returns:\n",
        "        macro_f1_over_axes, axis_metrics_dict\n",
        "    \"\"\"\n",
        "\n",
        "    ds = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices({\"input_ids\": ids, \"attention_mask\": mask})\n",
        "          .batch(batch_size)\n",
        "    )\n",
        "\n",
        "\n",
        "    yhat = model.predict(ds, verbose=0)  # (N,4)\n",
        "    y_pred_bin = (yhat >= thresh).astype(\"int32\")\n",
        "\n",
        "    axis_metrics = {}\n",
        "    macro_f1_per_axis = []\n",
        "\n",
        "    print(f\"\\n--- {run_name}: Per-axis metrics ---\")\n",
        "\n",
        "    for j, axis_name in enumerate(AXIS_DISPLAY_NAMES):\n",
        "        y_true_axis = y_axes[:, j]\n",
        "        y_pred_axis = y_pred_bin[:, j]\n",
        "\n",
        "        acc_axis      = accuracy_score(y_true_axis, y_pred_axis)\n",
        "        f1_macro_axis = f1_score(y_true_axis, y_pred_axis, average=\"macro\")\n",
        "\n",
        "        axis_metrics[axis_name] = {\n",
        "            \"accuracy\": acc_axis,\n",
        "            \"macroF1\": f1_macro_axis,\n",
        "        }\n",
        "        macro_f1_per_axis.append(f1_macro_axis)\n",
        "\n",
        "        print(f\"{axis_name}: acc={acc_axis:0.3f}  macroF1={f1_macro_axis:0.3f}\")\n",
        "\n",
        "    macro_f1_axes = float(np.mean(macro_f1_per_axis))\n",
        "    return macro_f1_axes, axis_metrics"
      ],
      "metadata": {
        "id": "KNLkcMRKRVCd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This will executre the four DistilBERT axis experiments and ΔF1 summary\n",
        "\n",
        "##1) Kaggle → Kaggle (in-domain)\n",
        "f1_k_in_axes, metrics_k_in_axes = eval_bert_axes_model(\n",
        "    m_k_axes,\n",
        "    Xk_ids_va, Xk_mask_va, yk_axes_va,\n",
        "    \"Kaggle → Kaggle (val)\"\n",
        ")\n",
        "\n",
        "##2) Reddit → Reddit (in-domain)\n",
        "f1_r_in_axes, metrics_r_in_axes = eval_bert_axes_model(\n",
        "    m_r_axes,\n",
        "    Xr_ids_va, Xr_mask_va, yr_axes_va,\n",
        "    \"Reddit → Reddit (val)\"\n",
        ")\n",
        "\n",
        "##3) Kaggle → Reddit (cross)\n",
        "f1_k2r_axes, metrics_k2r_axes = eval_bert_axes_model(\n",
        "    m_k_axes,\n",
        "    Xr_ids_va, Xr_mask_va, yr_axes_va,\n",
        "    \"Kaggle → Reddit (cross)\"\n",
        ")\n",
        "\n",
        "##4) Reddit → Kaggle (cross)\n",
        "f1_r2k_axes, metrics_r2k_axes = eval_bert_axes_model(\n",
        "    m_r_axes,\n",
        "    Xk_ids_va, Xk_mask_va, yk_axes_va,\n",
        "    \"Reddit → Kaggle (cross)\"\n",
        ")\n",
        "\n",
        "##ΔF1 summary\n",
        "print(\"\\n=== DistilBERT (axes) cross-domain drop (ΔF1 = in-domain − cross) ===\")\n",
        "print(f\"Kaggle-trained ΔF1 (axes): {f1_k_in_axes - f1_k2r_axes:.4f}\")\n",
        "print(f\"Reddit-trained ΔF1 (axes): {f1_r_in_axes - f1_r2k_axes:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "008YikTGRdOs",
        "outputId": "6cceb052-0c7c-49a0-ae41-2cc1ee8ab6c9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Kaggle → Kaggle (val): Per-axis metrics ---\n",
            "IE: acc=0.773  macroF1=0.436\n",
            "SN: acc=0.859  macroF1=0.462\n",
            "TF: acc=0.572  macroF1=0.526\n",
            "JP: acc=0.590  macroF1=0.373\n",
            "\n",
            "--- Reddit → Reddit (val): Per-axis metrics ---\n",
            "IE: acc=0.769  macroF1=0.435\n",
            "SN: acc=0.933  macroF1=0.483\n",
            "TF: acc=0.688  macroF1=0.417\n",
            "JP: acc=0.608  macroF1=0.389\n",
            "\n",
            "--- Kaggle → Reddit (cross): Per-axis metrics ---\n",
            "IE: acc=0.769  macroF1=0.435\n",
            "SN: acc=0.933  macroF1=0.483\n",
            "TF: acc=0.436  macroF1=0.431\n",
            "JP: acc=0.607  macroF1=0.385\n",
            "\n",
            "--- Reddit → Kaggle (cross): Per-axis metrics ---\n",
            "IE: acc=0.773  macroF1=0.436\n",
            "SN: acc=0.859  macroF1=0.462\n",
            "TF: acc=0.467  macroF1=0.323\n",
            "JP: acc=0.593  macroF1=0.375\n",
            "\n",
            "=== DistilBERT (axes) cross-domain drop (ΔF1 = in-domain − cross) ===\n",
            "Kaggle-trained ΔF1 (axes): 0.0160\n",
            "Reddit-trained ΔF1 (axes): 0.0320\n"
          ]
        }
      ]
    }
  ]
}