{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"tensorflow==2.19.*\" \"tensorflow-text==2.19.*\" \"tf-keras==2.19.*\" \"tensorflow-decision-forests==1.12.*\"\n",
        "import os, sys; os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "2FVF7zyRQeGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fudnKadRuqG",
        "outputId": "7e17347b-7ca4-467e-cb86-35a7d2cde3ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast, re, math, numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow.keras import layers, mixed_precision, Model\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "\n",
        "##This will configure GPU +AMP\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "for g in gpus:\n",
        "    try: tf.config.experimental.set_memory_growth(g, True)\n",
        "    except: pass\n",
        "mixed_precision.set_global_policy('float32')\n",
        "print(\"TF:\", tf.__version__, \"GPUs:\", gpus, \"AMP:\", mixed_precision.global_policy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7lNVzcSmBi",
        "outputId": "912c20b0-77ec-4c53-ed41-0a735efb7bc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0 GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] AMP: <DTypePolicy \"float32\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will load the datasets\n",
        "KAGGLE_CSV = \"kaggle_data.csv\"\n",
        "REDDIT_CSV = \"reddit_full.csv\"\n",
        "\n",
        "##This will set out 16 classifiers\n",
        "MBTI16 = ['ISTJ','ISFJ','INFJ','INTJ','ISTP','ISFP','INFP','INTP',\n",
        "          'ENTJ','ENTP','ENFJ','ENFP','ESTJ','ESFJ','ESTP','ESFP']\n",
        "lab2id = {l:i for i,l in enumerate(MBTI16)}\n",
        "\n",
        "def load_df(path, text_col_guess=(\"posts\",\"body\"), label_col_guess=(\"type\",\"class\")):\n",
        "    df = pd.read_csv(path)\n",
        "    ##This will figure out columns\n",
        "    text_col  = next((c for c in text_col_guess  if c in df.columns), None)\n",
        "    label_col = next((c for c in label_col_guess if c in df.columns), None)\n",
        "    assert text_col and label_col, f\"Could not find text/label in {path}. Columns: {df.columns.tolist()}\"\n",
        "\n",
        "    def liststr_to_str(x):\n",
        "        if isinstance(x, str) and x.startswith('['):\n",
        "            try:\n",
        "                toks = ast.literal_eval(x)\n",
        "                if isinstance(toks, list): return \" \".join(map(str, toks))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return str(x)\n",
        "\n",
        "    df = df[[text_col, label_col]].rename(columns={text_col:'text', label_col:'label'})\n",
        "    df['text']  = df['text'].map(liststr_to_str)\n",
        "    df['label'] = df['label'].astype(str)\n",
        "    df = df[df['label'].isin(MBTI16)].copy()\n",
        "    df['y'] = df['label'].map(lab2id).astype(int)\n",
        "    return df\n",
        "\n",
        "df_k = load_df(KAGGLE_CSV, text_col_guess=(\"posts\",\"body\"),  label_col_guess=(\"type\",\"class\"))\n",
        "df_r = load_df(REDDIT_CSV, text_col_guess=(\"body\",\"posts\"),  label_col_guess=(\"class\",\"type\"))\n",
        "print(\"Kaggle rows:\", len(df_k), \"Reddit rows:\", len(df_r))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQsHPSEFSnjt",
        "outputId": "56094122-1ea3-46cb-ae7c-873a2bd9c28f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle rows: 410915 Reddit rows: 1651100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "VOCAB_SZ = 20000\n",
        "MAX_LEN  = 160\n",
        "\n",
        "tok = Tokenizer(num_words=VOCAB_SZ, oov_token=\"<UNK>\")\n",
        "tok.fit_on_texts(pd.concat([df_k['text'], df_r['text']], axis=0))\n",
        "\n",
        "def featurize(texts):\n",
        "    seq = tok.texts_to_sequences(texts)\n",
        "    return pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "Xk, yk = featurize(df_k['text']), df_k['y'].to_numpy()\n",
        "Xr, yr = featurize(df_r['text']), df_r['y'].to_numpy()\n"
      ],
      "metadata": {
        "id": "QggoackBTXqg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH = 128 if gpus else 32\n",
        "\n",
        "def make_split(X, y, frac=0.9, shuffle=True):\n",
        "    n = len(X); k = int(n*frac)\n",
        "    Xtr, Xva = X[:k], X[k:]\n",
        "    ytr, yva = y[:k], y[k:]\n",
        "    if shuffle:\n",
        "        idx = np.random.permutation(len(Xtr))\n",
        "        Xtr, ytr = Xtr[idx], ytr[idx]\n",
        "    ds_tr = tf.data.Dataset.from_tensor_slices((Xtr, ytr)).batch(BATCH).prefetch(AUTOTUNE)\n",
        "    ds_va = tf.data.Dataset.from_tensor_slices((Xva, yva)).batch(BATCH).prefetch(AUTOTUNE)\n",
        "    return ds_tr, ds_va, (Xva, yva)\n",
        "\n",
        "ds_k_tr, ds_k_va, (Xk_va, yk_va) = make_split(Xk, yk)\n",
        "ds_r_tr, ds_r_va, (Xr_va, yr_va) = make_split(Xr, yr)\n"
      ],
      "metadata": {
        "id": "0S8UNo9GTwFB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This will establish our CNN function\n",
        "def make_cnn(num_classes=16, vocab_size=VOCAB_SZ, embed_dim=96, seq_len=MAX_LEN, filters=192, drop=0.3):\n",
        "    inp = layers.Input((seq_len,), name=\"input_ids\")\n",
        "    emb = layers.Embedding(vocab_size, embed_dim, name=\"embedding\")(inp)\n",
        "    c3  = layers.Conv1D(filters, 3, activation=\"relu\")(emb)\n",
        "    c4  = layers.Conv1D(filters, 4, activation=\"relu\")(emb)\n",
        "    c5  = layers.Conv1D(filters, 5, activation=\"relu\")(emb)\n",
        "    p3, p4, p5 = layers.GlobalMaxPooling1D()(c3), layers.GlobalMaxPooling1D()(c4), layers.GlobalMaxPooling1D()(c5)\n",
        "    x = layers.Concatenate()([p3,p4,p5])\n",
        "    x = layers.Dropout(drop)(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(drop)(x)\n",
        "    ##This will keep logits/probs in float32 for numerical stability with AMP\n",
        "    out = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "    m = Model(inp, out)\n",
        "    m.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(2e-4),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return m\n",
        "\n",
        "cb = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "zgYfnM5UT-gb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This will train the function in Kaggle/Reddit datasets\n",
        "model_k = make_cnn()\n",
        "print(\"=== Train on Kaggle ===\")\n",
        "hist_k = model_k.fit(ds_k_tr, validation_data=ds_k_va, epochs=6, verbose=1)\n",
        "\n",
        "model_r = make_cnn()\n",
        "print(\"\\n=== Train on Reddit ===\")\n",
        "hist_r = model_r.fit(ds_r_tr, validation_data=ds_r_va, epochs=6, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVH43-ySUBZr",
        "outputId": "a683a012-734b-4daa-961e-ef0e6355aa18"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train on Kaggle ===\n",
            "Epoch 1/6\n",
            "\u001b[1m2890/2890\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 17ms/step - accuracy: 0.2042 - loss: 2.3118 - val_accuracy: 0.2279 - val_loss: 2.2233\n",
            "Epoch 2/6\n",
            "\u001b[1m2890/2890\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - accuracy: 0.2349 - loss: 2.2354 - val_accuracy: 0.2301 - val_loss: 2.2204\n",
            "Epoch 3/6\n",
            "\u001b[1m2890/2890\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - accuracy: 0.2571 - loss: 2.1891 - val_accuracy: 0.2264 - val_loss: 2.2342\n",
            "Epoch 4/6\n",
            "\u001b[1m2890/2890\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - accuracy: 0.2808 - loss: 2.1354 - val_accuracy: 0.2255 - val_loss: 2.2612\n",
            "Epoch 5/6\n",
            "\u001b[1m2890/2890\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - accuracy: 0.3090 - loss: 2.0648 - val_accuracy: 0.2199 - val_loss: 2.3038\n",
            "Epoch 6/6\n",
            "\u001b[1m2890/2890\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - accuracy: 0.3453 - loss: 1.9714 - val_accuracy: 0.2134 - val_loss: 2.3747\n",
            "\n",
            "=== Train on Reddit ===\n",
            "Epoch 1/6\n",
            "\u001b[1m11610/11610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 15ms/step - accuracy: 0.2813 - loss: 2.0267 - val_accuracy: 0.3169 - val_loss: 1.9361\n",
            "Epoch 2/6\n",
            "\u001b[1m11610/11610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 14ms/step - accuracy: 0.3170 - loss: 1.9375 - val_accuracy: 0.3214 - val_loss: 1.9229\n",
            "Epoch 3/6\n",
            "\u001b[1m11610/11610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 14ms/step - accuracy: 0.3303 - loss: 1.9046 - val_accuracy: 0.3213 - val_loss: 1.9215\n",
            "Epoch 4/6\n",
            "\u001b[1m11610/11610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 14ms/step - accuracy: 0.3417 - loss: 1.8737 - val_accuracy: 0.3198 - val_loss: 1.9268\n",
            "Epoch 5/6\n",
            "\u001b[1m11610/11610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 14ms/step - accuracy: 0.3541 - loss: 1.8402 - val_accuracy: 0.3169 - val_loss: 1.9407\n",
            "Epoch 6/6\n",
            "\u001b[1m11610/11610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 14ms/step - accuracy: 0.3668 - loss: 1.8066 - val_accuracy: 0.3135 - val_loss: 1.9603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This will create an eval report of the model\n",
        "def eval_report(model, X, y, tag):\n",
        "    yhat = model.predict(tf.data.Dataset.from_tensor_slices(X).batch(BATCH), verbose=0)\n",
        "    pred = np.argmax(yhat, axis=1)\n",
        "    print(f\"\\n--- {tag} ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y, pred))\n",
        "    print(\"Macro-F1:\", f1_score(y, pred, average='macro'))\n",
        "    print(classification_report(y, pred, target_names=MBTI16, digits=3))\n",
        "    return accuracy_score(y, pred), f1_score(y, pred, average='macro')\n",
        "\n",
        "##This will be in-domain\n",
        "acc_k_in, f1_k_in = eval_report(model_k, Xk_va, yk_va, \"Kaggle → Kaggle (val)\")\n",
        "acc_r_in, f1_r_in = eval_report(model_r, Xr_va, yr_va, \"Reddit → Reddit (val)\")\n",
        "\n",
        "##This will be cross-domain\n",
        "acc_k2r, f1_k2r = eval_report(model_k, Xr_va, yr_va, \"Kaggle → Reddit (cross)\")\n",
        "acc_r2k, f1_r2k = eval_report(model_r, Xk_va, yk_va, \"Reddit → Kaggle (cross)\")\n",
        "\n",
        "print(\"\\n=== Cross-domain drop (ΔF1 = within − cross) ===\")\n",
        "print(f\"Kaggle-trained ΔF1: {f1_k_in - f1_k2r:.4f}\")\n",
        "print(f\"Reddit-trained ΔF1: {f1_r_in - f1_r2k:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBUp5Ei6VswC",
        "outputId": "938a93cd-f070-4e3e-f9fd-58bc6ab5e7b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Kaggle → Kaggle (val) ---\n",
            "Accuracy: 0.21337486615399592\n",
            "Macro-F1: 0.07329459619773335\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.226     0.006     0.012      1093\n",
            "        ISFJ      0.000     0.000     0.000       642\n",
            "        INFJ      0.213     0.251     0.230      7245\n",
            "        INTJ      0.162     0.146     0.154      5108\n",
            "        ISTP      0.117     0.018     0.031      1511\n",
            "        ISFP      0.333     0.001     0.002      1262\n",
            "        INFP      0.246     0.495     0.328      8495\n",
            "        INTP      0.218     0.225     0.222      6441\n",
            "        ENTJ      0.077     0.001     0.002      1271\n",
            "        ENTP      0.115     0.071     0.088      2963\n",
            "        ENFJ      0.000     0.000     0.000       693\n",
            "        ENFP      0.145     0.082     0.104      3689\n",
            "        ESTJ      0.000     0.000     0.000       196\n",
            "        ESFJ      0.000     0.000     0.000       185\n",
            "        ESTP      0.000     0.000     0.000       250\n",
            "        ESFP      0.000     0.000     0.000        48\n",
            "\n",
            "    accuracy                          0.213     41092\n",
            "   macro avg      0.116     0.081     0.073     41092\n",
            "weighted avg      0.187     0.213     0.180     41092\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Reddit → Reddit (val) ---\n",
            "Accuracy: 0.3135000908485252\n",
            "Macro-F1: 0.10472101921417397\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.750     0.002     0.004      1660\n",
            "        ISFJ      0.000     0.000     0.000       694\n",
            "        INFJ      0.280     0.201     0.234     19396\n",
            "        INTJ      0.319     0.301     0.310     35773\n",
            "        ISTP      0.282     0.013     0.025      5081\n",
            "        ISFP      0.580     0.026     0.051      1097\n",
            "        INFP      0.264     0.174     0.210     17777\n",
            "        INTP      0.331     0.657     0.440     45230\n",
            "        ENTJ      0.312     0.020     0.038      4259\n",
            "        ENTP      0.281     0.169     0.211     19716\n",
            "        ENFJ      0.277     0.025     0.045      2077\n",
            "        ENFP      0.215     0.073     0.109      9699\n",
            "        ESTJ      0.000     0.000     0.000       429\n",
            "        ESFJ      0.000     0.000     0.000       272\n",
            "        ESTP      0.000     0.000     0.000      1210\n",
            "        ESFP      0.000     0.000     0.000       740\n",
            "\n",
            "    accuracy                          0.314    165110\n",
            "   macro avg      0.243     0.104     0.105    165110\n",
            "weighted avg      0.299     0.314     0.272    165110\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Kaggle → Reddit (cross) ---\n",
            "Accuracy: 0.18839561504451577\n",
            "Macro-F1: 0.06429544744112084\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.000     0.000     0.000      1660\n",
            "        ISFJ      0.000     0.000     0.000       694\n",
            "        INFJ      0.159     0.281     0.203     19396\n",
            "        INTJ      0.277     0.122     0.169     35773\n",
            "        ISTP      0.098     0.014     0.024      5081\n",
            "        ISFP      0.000     0.000     0.000      1097\n",
            "        INFP      0.128     0.523     0.206     17777\n",
            "        INTP      0.338     0.227     0.271     45230\n",
            "        ENTJ      0.107     0.001     0.001      4259\n",
            "        ENTP      0.190     0.064     0.095     19716\n",
            "        ENFJ      0.000     0.000     0.000      2077\n",
            "        ENFP      0.085     0.044     0.058      9699\n",
            "        ESTJ      0.000     0.000     0.000       429\n",
            "        ESFJ      0.000     0.000     0.000       272\n",
            "        ESTP      0.000     0.000     0.000      1210\n",
            "        ESFP      0.000     0.000     0.000       740\n",
            "\n",
            "    accuracy                          0.188    165110\n",
            "   macro avg      0.086     0.080     0.064    165110\n",
            "weighted avg      0.219     0.188     0.173    165110\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Reddit → Kaggle (cross) ---\n",
            "Accuracy: 0.18619195950549985\n",
            "Macro-F1: 0.06344068883952056\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.000     0.000     0.000      1093\n",
            "        ISFJ      0.000     0.000     0.000       642\n",
            "        INFJ      0.243     0.120     0.160      7245\n",
            "        INTJ      0.149     0.242     0.184      5108\n",
            "        ISTP      0.121     0.005     0.009      1511\n",
            "        ISFP      0.111     0.001     0.002      1262\n",
            "        INFP      0.300     0.140     0.191      8495\n",
            "        INTP      0.181     0.603     0.278      6441\n",
            "        ENTJ      0.064     0.002     0.005      1271\n",
            "        ENTP      0.112     0.102     0.107      2963\n",
            "        ENFJ      0.077     0.006     0.011       693\n",
            "        ENFP      0.176     0.043     0.069      3689\n",
            "        ESTJ      0.000     0.000     0.000       196\n",
            "        ESFJ      0.000     0.000     0.000       185\n",
            "        ESTP      0.000     0.000     0.000       250\n",
            "        ESFP      0.000     0.000     0.000        48\n",
            "\n",
            "    accuracy                          0.186     41092\n",
            "   macro avg      0.096     0.079     0.063     41092\n",
            "weighted avg      0.187     0.186     0.149     41092\n",
            "\n",
            "\n",
            "=== Cross-domain drop (ΔF1 = within − cross) ===\n",
            "Kaggle-trained ΔF1: 0.0090\n",
            "Reddit-trained ΔF1: 0.0413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECOND PART**"
      ],
      "metadata": {
        "id": "5EGNk1A4Wn_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "##This will take the int labels 0..15 in MBTI16 order:\n",
        "MBTI16 = ['ISTJ','ISFJ','INFJ','INTJ','ISTP','ISFP','INFP','INTP','ENTJ','ENTP','ENFJ','ENFP','ESTJ','ESFJ','ESTP','ESFP']\n",
        "lab2id = {l:i for i,l in enumerate(MBTI16)}\n",
        "\n",
        "##This will split one type into 4 bits: I/E, S/N, T/F, J/P (1=the second letter)\n",
        "def type_to_axes(idx):\n",
        "    t = MBTI16[idx]\n",
        "    return (\n",
        "        0 if t[0]=='I' else 1,\n",
        "        0 if t[1]=='S' else 1,\n",
        "        0 if t[2]=='T' else 1,\n",
        "        0 if t[3]=='J' else 1\n",
        "    )\n",
        "\n",
        "def labels_to_axes(y):\n",
        "    axes = np.array([type_to_axes(i) for i in y], dtype=np.int32)\n",
        "    ##The shape here: (N, 4) columns = [IE, SN, TF, JP]\n",
        "    return axes\n",
        "\n",
        "yk_axes = labels_to_axes(yk)\n",
        "yr_axes = labels_to_axes(yr)\n"
      ],
      "metadata": {
        "id": "3JCbX6EqWska"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def axis_class_weights(y_axes):\n",
        "    weights = []\n",
        "    for j in range(4):\n",
        "        w = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=y_axes[:,j])\n",
        "        weights.append({0: float(w[0]), 1: float(w[1])})\n",
        "    return weights\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH = 256 if tf.config.list_physical_devices('GPU') else 32\n",
        "\n",
        "def make_axis_ds_with_weights(X, y_axes, wdicts, val_frac=0.1):\n",
        "    ##This will stratify by 16 way code reconstructed from axes\n",
        "    code16 = y_axes[:,0]*8 + y_axes[:,1]*4 + y_axes[:,2]*2 + y_axes[:,3]\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_frac, random_state=42)\n",
        "    idx_tr, idx_va = next(sss.split(X, code16))\n",
        "\n",
        "    Xtr, Xva = X[idx_tr], X[idx_va]\n",
        "    ytr, yva = y_axes[idx_tr], y_axes[idx_va]\n",
        "\n",
        "    ##This will set y dicts\n",
        "    def to_y_dict(Y):\n",
        "        Y = Y.astype(np.float32)\n",
        "        return {\"ie\": Y[:,0], \"sn\": Y[:,1], \"tf\": Y[:,2], \"jp\": Y[:,3]}\n",
        "\n",
        "    ytr_dict = to_y_dict(ytr)\n",
        "    yva_dict = to_y_dict(yva)\n",
        "\n",
        "    ##This will be the sample weights as ordered tuples (ie, sn, tf, jp)\n",
        "    ##training weights for example\n",
        "    Wtr = np.empty_like(ytr, dtype=np.float32)\n",
        "    for j in range(4):\n",
        "        Wtr[:, j] = np.where(ytr[:, j]==1, wdicts[j][1], wdicts[j][0])\n",
        "    sw_tr_tuple = (Wtr[:,0], Wtr[:,1], Wtr[:,2], Wtr[:,3])\n",
        "\n",
        "    ##Here validation weights = ones\n",
        "    sw_va_tuple = (\n",
        "        np.ones(len(yva), np.float32),\n",
        "        np.ones(len(yva), np.float32),\n",
        "        np.ones(len(yva), np.float32),\n",
        "        np.ones(len(yva), np.float32),\n",
        "    )\n",
        "\n",
        "    ds_tr = tf.data.Dataset.from_tensor_slices((Xtr, ytr_dict, sw_tr_tuple)) \\\n",
        "                           .shuffle(10000).batch(BATCH).prefetch(AUTOTUNE)\n",
        "    ds_va = tf.data.Dataset.from_tensor_slices((Xva, yva_dict, sw_va_tuple)) \\\n",
        "                           .batch(BATCH).prefetch(AUTOTUNE)\n",
        "    return ds_tr, ds_va, (Xva, yva)\n",
        "\n",
        "##This will rebuild the datasets\n",
        "cw_k_axes = axis_class_weights(yk_axes)\n",
        "cw_r_axes = axis_class_weights(yr_axes)\n",
        "ds_k_tr, ds_k_va, (Xk_va, yk_axes_va) = make_axis_ds_with_weights(Xk, yk_axes, cw_k_axes)\n",
        "ds_r_tr, ds_r_va, (Xr_va, yr_axes_va) = make_axis_ds_with_weights(Xr, yr_axes, cw_r_axes)"
      ],
      "metadata": {
        "id": "TrrsOY2PXmms"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "def make_cnn_axes(vocab_size=20000, embed_dim=96, seq_len=160, filters=192, drop=0.3):\n",
        "    inp = layers.Input((seq_len,), name=\"input_ids\")\n",
        "    emb = layers.Embedding(vocab_size, embed_dim, name=\"embedding\")(inp)\n",
        "    c3  = layers.Conv1D(filters, 3, activation=\"relu\")(emb)\n",
        "    c4  = layers.Conv1D(filters, 4, activation=\"relu\")(emb)\n",
        "    c5  = layers.Conv1D(filters, 5, activation=\"relu\")(emb)\n",
        "    x   = layers.Concatenate()([\n",
        "        layers.GlobalMaxPooling1D()(c3),\n",
        "        layers.GlobalMaxPooling1D()(c4),\n",
        "        layers.GlobalMaxPooling1D()(c5),\n",
        "    ])\n",
        "    x   = layers.Dropout(drop)(x)\n",
        "    x   = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x   = layers.Dropout(drop)(x)\n",
        "    ##This will use 4 independent binary heads and keepss outputs in float32 for AMP stability\n",
        "    out_ie = layers.Dense(1, activation=\"sigmoid\", name=\"ie\", dtype=\"float32\")(x)\n",
        "    out_sn = layers.Dense(1, activation=\"sigmoid\", name=\"sn\", dtype=\"float32\")(x)\n",
        "    out_tf = layers.Dense(1, activation=\"sigmoid\", name=\"tf\", dtype=\"float32\")(x)\n",
        "    out_jp = layers.Dense(1, activation=\"sigmoid\", name=\"jp\", dtype=\"float32\")(x)\n",
        "    m = Model(inp, [out_ie, out_sn, out_tf, out_jp])\n",
        "    m.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss={\"ie\":\"binary_crossentropy\",\"sn\":\"binary_crossentropy\",\n",
        "              \"tf\":\"binary_crossentropy\",\"jp\":\"binary_crossentropy\"},\n",
        "        metrics={\"ie\":[\"accuracy\"],\"sn\":[\"accuracy\"],\"tf\":[\"accuracy\"],\"jp\":[\"accuracy\"]},\n",
        "    )\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "UD8yCkB_WxrY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
        "]\n",
        "\n",
        "m_k = make_cnn_axes(vocab_size=VOCAB_SZ, seq_len=MAX_LEN)\n",
        "print(\"=== Train axes on Kaggle ===\")\n",
        "m_k.fit(ds_k_tr, validation_data=ds_k_va, epochs=12, callbacks=cb, verbose=1)\n",
        "\n",
        "m_r = make_cnn_axes(vocab_size=VOCAB_SZ, seq_len=MAX_LEN)\n",
        "print(\"\\n=== Train axes on Reddit ===\")\n",
        "m_r.fit(ds_r_tr, validation_data=ds_r_va, epochs=12, callbacks=cb, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAdqy4E7W0jh",
        "outputId": "a5f7a51e-b22b-4564-ef0a-bdf430c032ec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train axes on Kaggle ===\n",
            "Epoch 1/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 35ms/step - ie_accuracy: 0.5148 - ie_loss: 0.6930 - jp_accuracy: 0.5073 - jp_loss: 0.6930 - loss: 2.7718 - sn_accuracy: 0.5639 - sn_loss: 0.6932 - tf_accuracy: 0.5131 - tf_loss: 0.6927 - val_ie_accuracy: 0.6305 - val_ie_loss: 0.6860 - val_jp_accuracy: 0.5332 - val_jp_loss: 0.6910 - val_loss: 2.7369 - val_sn_accuracy: 0.6842 - val_sn_loss: 0.6778 - val_tf_accuracy: 0.5577 - val_tf_loss: 0.6821 - learning_rate: 1.0000e-04\n",
            "Epoch 2/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 30ms/step - ie_accuracy: 0.5259 - ie_loss: 0.6904 - jp_accuracy: 0.5241 - jp_loss: 0.6917 - loss: 2.7476 - sn_accuracy: 0.5483 - sn_loss: 0.6894 - tf_accuracy: 0.5699 - tf_loss: 0.6761 - val_ie_accuracy: 0.5025 - val_ie_loss: 0.6967 - val_jp_accuracy: 0.5259 - val_jp_loss: 0.6916 - val_loss: 2.7394 - val_sn_accuracy: 0.5153 - val_sn_loss: 0.6908 - val_tf_accuracy: 0.6012 - val_tf_loss: 0.6603 - learning_rate: 1.0000e-04\n",
            "Epoch 3/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 30ms/step - ie_accuracy: 0.5635 - ie_loss: 0.6818 - jp_accuracy: 0.5475 - jp_loss: 0.6881 - loss: 2.6987 - sn_accuracy: 0.5537 - sn_loss: 0.6749 - tf_accuracy: 0.6110 - tf_loss: 0.6539 - val_ie_accuracy: 0.5686 - val_ie_loss: 0.6816 - val_jp_accuracy: 0.5398 - val_jp_loss: 0.6890 - val_loss: 2.7084 - val_sn_accuracy: 0.5371 - val_sn_loss: 0.6805 - val_tf_accuracy: 0.6057 - val_tf_loss: 0.6574 - learning_rate: 1.0000e-04\n",
            "Epoch 4/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - ie_accuracy: 0.5914 - ie_loss: 0.6708 - jp_accuracy: 0.5651 - jp_loss: 0.6823 - loss: 2.6483 - sn_accuracy: 0.5809 - sn_loss: 0.6533 - tf_accuracy: 0.6284 - tf_loss: 0.6419 - val_ie_accuracy: 0.5865 - val_ie_loss: 0.6725 - val_jp_accuracy: 0.5361 - val_jp_loss: 0.6916 - val_loss: 2.6914 - val_sn_accuracy: 0.5604 - val_sn_loss: 0.6668 - val_tf_accuracy: 0.6026 - val_tf_loss: 0.6606 - learning_rate: 1.0000e-04\n",
            "Epoch 5/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - ie_accuracy: 0.6097 - ie_loss: 0.6593 - jp_accuracy: 0.5818 - jp_loss: 0.6751 - loss: 2.5984 - sn_accuracy: 0.6013 - sn_loss: 0.6313 - tf_accuracy: 0.6390 - tf_loss: 0.6327 - val_ie_accuracy: 0.5980 - val_ie_loss: 0.6649 - val_jp_accuracy: 0.5417 - val_jp_loss: 0.6917 - val_loss: 2.6901 - val_sn_accuracy: 0.5549 - val_sn_loss: 0.6704 - val_tf_accuracy: 0.6026 - val_tf_loss: 0.6632 - learning_rate: 1.0000e-04\n",
            "Epoch 6/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - ie_accuracy: 0.6230 - ie_loss: 0.6475 - jp_accuracy: 0.5944 - jp_loss: 0.6686 - loss: 2.5498 - sn_accuracy: 0.6221 - sn_loss: 0.6083 - tf_accuracy: 0.6477 - tf_loss: 0.6254 - val_ie_accuracy: 0.5729 - val_ie_loss: 0.6816 - val_jp_accuracy: 0.5463 - val_jp_loss: 0.6924 - val_loss: 2.7085 - val_sn_accuracy: 0.5622 - val_sn_loss: 0.6688 - val_tf_accuracy: 0.6035 - val_tf_loss: 0.6658 - learning_rate: 1.0000e-04\n",
            "Epoch 7/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - ie_accuracy: 0.6324 - ie_loss: 0.6360 - jp_accuracy: 0.6045 - jp_loss: 0.6616 - loss: 2.4989 - sn_accuracy: 0.6464 - sn_loss: 0.5831 - tf_accuracy: 0.6549 - tf_loss: 0.6181\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 30ms/step - ie_accuracy: 0.6324 - ie_loss: 0.6360 - jp_accuracy: 0.6045 - jp_loss: 0.6616 - loss: 2.4988 - sn_accuracy: 0.6464 - sn_loss: 0.5831 - tf_accuracy: 0.6549 - tf_loss: 0.6181 - val_ie_accuracy: 0.5797 - val_ie_loss: 0.6782 - val_jp_accuracy: 0.5495 - val_jp_loss: 0.6932 - val_loss: 2.6936 - val_sn_accuracy: 0.5859 - val_sn_loss: 0.6524 - val_tf_accuracy: 0.5999 - val_tf_loss: 0.6698 - learning_rate: 1.0000e-04\n",
            "Epoch 8/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - ie_accuracy: 0.6497 - ie_loss: 0.6213 - jp_accuracy: 0.6154 - jp_loss: 0.6527 - loss: 2.4280 - sn_accuracy: 0.6803 - sn_loss: 0.5455 - tf_accuracy: 0.6633 - tf_loss: 0.6084 - val_ie_accuracy: 0.5679 - val_ie_loss: 0.6927 - val_jp_accuracy: 0.5519 - val_jp_loss: 0.6951 - val_loss: 2.6940 - val_sn_accuracy: 0.6168 - val_sn_loss: 0.6328 - val_tf_accuracy: 0.6010 - val_tf_loss: 0.6735 - learning_rate: 5.0000e-05\n",
            "Epoch 9/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - ie_accuracy: 0.6570 - ie_loss: 0.6119 - jp_accuracy: 0.6209 - jp_loss: 0.6491 - loss: 2.3861 - sn_accuracy: 0.6988 - sn_loss: 0.5220 - tf_accuracy: 0.6682 - tf_loss: 0.6030 - val_ie_accuracy: 0.5858 - val_ie_loss: 0.6815 - val_jp_accuracy: 0.5455 - val_jp_loss: 0.7007 - val_loss: 2.6792 - val_sn_accuracy: 0.6338 - val_sn_loss: 0.6225 - val_tf_accuracy: 0.6011 - val_tf_loss: 0.6746 - learning_rate: 5.0000e-05\n",
            "Epoch 10/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - ie_accuracy: 0.6642 - ie_loss: 0.6047 - jp_accuracy: 0.6260 - jp_loss: 0.6444 - loss: 2.3438 - sn_accuracy: 0.7199 - sn_loss: 0.4962 - tf_accuracy: 0.6731 - tf_loss: 0.5985 - val_ie_accuracy: 0.6003 - val_ie_loss: 0.6725 - val_jp_accuracy: 0.5412 - val_jp_loss: 0.7051 - val_loss: 2.6789 - val_sn_accuracy: 0.6382 - val_sn_loss: 0.6239 - val_tf_accuracy: 0.5992 - val_tf_loss: 0.6775 - learning_rate: 5.0000e-05\n",
            "Epoch 11/12\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - ie_accuracy: 0.6719 - ie_loss: 0.5950 - jp_accuracy: 0.6307 - jp_loss: 0.6406 - loss: 2.3036 - sn_accuracy: 0.7349 - sn_loss: 0.4742 - tf_accuracy: 0.6769 - tf_loss: 0.5938 - val_ie_accuracy: 0.5903 - val_ie_loss: 0.6821 - val_jp_accuracy: 0.5480 - val_jp_loss: 0.7023 - val_loss: 2.7000 - val_sn_accuracy: 0.6337 - val_sn_loss: 0.6354 - val_tf_accuracy: 0.5986 - val_tf_loss: 0.6802 - learning_rate: 5.0000e-05\n",
            "Epoch 12/12\n",
            "\u001b[1m1443/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - ie_accuracy: 0.6785 - ie_loss: 0.5871 - jp_accuracy: 0.6367 - jp_loss: 0.6354 - loss: 2.2605 - sn_accuracy: 0.7512 - sn_loss: 0.4492 - tf_accuracy: 0.6820 - tf_loss: 0.5888\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - ie_accuracy: 0.6785 - ie_loss: 0.5871 - jp_accuracy: 0.6367 - jp_loss: 0.6354 - loss: 2.2605 - sn_accuracy: 0.7512 - sn_loss: 0.4492 - tf_accuracy: 0.6820 - tf_loss: 0.5888 - val_ie_accuracy: 0.5924 - val_ie_loss: 0.6848 - val_jp_accuracy: 0.5480 - val_jp_loss: 0.7041 - val_loss: 2.6829 - val_sn_accuracy: 0.6642 - val_sn_loss: 0.6105 - val_tf_accuracy: 0.5976 - val_tf_loss: 0.6834 - learning_rate: 5.0000e-05\n",
            "\n",
            "=== Train axes on Reddit ===\n",
            "Epoch 1/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 30ms/step - ie_accuracy: 0.5257 - ie_loss: 0.6900 - jp_accuracy: 0.5286 - jp_loss: 0.6898 - loss: 2.7409 - sn_accuracy: 0.5562 - sn_loss: 0.6847 - tf_accuracy: 0.5733 - tf_loss: 0.6764 - val_ie_accuracy: 0.5897 - val_ie_loss: 0.6695 - val_jp_accuracy: 0.5569 - val_jp_loss: 0.6845 - val_loss: 2.6734 - val_sn_accuracy: 0.5547 - val_sn_loss: 0.6817 - val_tf_accuracy: 0.6420 - val_tf_loss: 0.6377 - learning_rate: 1.0000e-04\n",
            "Epoch 2/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 29ms/step - ie_accuracy: 0.5822 - ie_loss: 0.6716 - jp_accuracy: 0.5735 - jp_loss: 0.6790 - loss: 2.6373 - sn_accuracy: 0.6002 - sn_loss: 0.6509 - tf_accuracy: 0.6446 - tf_loss: 0.6357 - val_ie_accuracy: 0.5779 - val_ie_loss: 0.6686 - val_jp_accuracy: 0.5926 - val_jp_loss: 0.6705 - val_loss: 2.6304 - val_sn_accuracy: 0.5541 - val_sn_loss: 0.6733 - val_tf_accuracy: 0.6657 - val_tf_loss: 0.6179 - learning_rate: 1.0000e-04\n",
            "Epoch 3/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 29ms/step - ie_accuracy: 0.5879 - ie_loss: 0.6611 - jp_accuracy: 0.5912 - jp_loss: 0.6718 - loss: 2.5918 - sn_accuracy: 0.6068 - sn_loss: 0.6320 - tf_accuracy: 0.6554 - tf_loss: 0.6269 - val_ie_accuracy: 0.5773 - val_ie_loss: 0.6653 - val_jp_accuracy: 0.5917 - val_jp_loss: 0.6708 - val_loss: 2.6345 - val_sn_accuracy: 0.5507 - val_sn_loss: 0.6704 - val_tf_accuracy: 0.6522 - val_tf_loss: 0.6280 - learning_rate: 1.0000e-04\n",
            "Epoch 4/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 29ms/step - ie_accuracy: 0.5910 - ie_loss: 0.6547 - jp_accuracy: 0.5999 - jp_loss: 0.6668 - loss: 2.5563 - sn_accuracy: 0.6166 - sn_loss: 0.6133 - tf_accuracy: 0.6610 - tf_loss: 0.6216 - val_ie_accuracy: 0.5839 - val_ie_loss: 0.6601 - val_jp_accuracy: 0.5876 - val_jp_loss: 0.6726 - val_loss: 2.5769 - val_sn_accuracy: 0.6173 - val_sn_loss: 0.6115 - val_tf_accuracy: 0.6480 - val_tf_loss: 0.6327 - learning_rate: 1.0000e-04\n",
            "Epoch 5/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 29ms/step - ie_accuracy: 0.5962 - ie_loss: 0.6492 - jp_accuracy: 0.6057 - jp_loss: 0.6633 - loss: 2.5154 - sn_accuracy: 0.6379 - sn_loss: 0.5853 - tf_accuracy: 0.6650 - tf_loss: 0.6175 - val_ie_accuracy: 0.5767 - val_ie_loss: 0.6651 - val_jp_accuracy: 0.5853 - val_jp_loss: 0.6744 - val_loss: 2.5359 - val_sn_accuracy: 0.6692 - val_sn_loss: 0.5592 - val_tf_accuracy: 0.6445 - val_tf_loss: 0.6371 - learning_rate: 1.0000e-04\n",
            "Epoch 6/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 29ms/step - ie_accuracy: 0.6010 - ie_loss: 0.6443 - jp_accuracy: 0.6100 - jp_loss: 0.6606 - loss: 2.4632 - sn_accuracy: 0.6700 - sn_loss: 0.5438 - tf_accuracy: 0.6678 - tf_loss: 0.6145 - val_ie_accuracy: 0.5909 - val_ie_loss: 0.6544 - val_jp_accuracy: 0.5845 - val_jp_loss: 0.6759 - val_loss: 2.5750 - val_sn_accuracy: 0.6169 - val_sn_loss: 0.6163 - val_tf_accuracy: 0.6531 - val_tf_loss: 0.6284 - learning_rate: 1.0000e-04\n",
            "Epoch 7/12\n",
            "\u001b[1m5803/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - ie_accuracy: 0.6048 - ie_loss: 0.6401 - jp_accuracy: 0.6145 - jp_loss: 0.6580 - loss: 2.4059 - sn_accuracy: 0.7009 - sn_loss: 0.4961 - tf_accuracy: 0.6717 - tf_loss: 0.6117\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 29ms/step - ie_accuracy: 0.6048 - ie_loss: 0.6401 - jp_accuracy: 0.6145 - jp_loss: 0.6580 - loss: 2.4059 - sn_accuracy: 0.7009 - sn_loss: 0.4961 - tf_accuracy: 0.6717 - tf_loss: 0.6117 - val_ie_accuracy: 0.5785 - val_ie_loss: 0.6636 - val_jp_accuracy: 0.5862 - val_jp_loss: 0.6752 - val_loss: 2.5362 - val_sn_accuracy: 0.6617 - val_sn_loss: 0.5681 - val_tf_accuracy: 0.6520 - val_tf_loss: 0.6293 - learning_rate: 1.0000e-04\n",
            "Epoch 8/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 29ms/step - ie_accuracy: 0.6105 - ie_loss: 0.6349 - jp_accuracy: 0.6178 - jp_loss: 0.6553 - loss: 2.3449 - sn_accuracy: 0.7349 - sn_loss: 0.4461 - tf_accuracy: 0.6737 - tf_loss: 0.6086 - val_ie_accuracy: 0.5922 - val_ie_loss: 0.6546 - val_jp_accuracy: 0.5809 - val_jp_loss: 0.6794 - val_loss: 2.4487 - val_sn_accuracy: 0.7395 - val_sn_loss: 0.4822 - val_tf_accuracy: 0.6496 - val_tf_loss: 0.6326 - learning_rate: 5.0000e-05\n",
            "Epoch 9/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 29ms/step - ie_accuracy: 0.6137 - ie_loss: 0.6312 - jp_accuracy: 0.6197 - jp_loss: 0.6536 - loss: 2.3065 - sn_accuracy: 0.7564 - sn_loss: 0.4153 - tf_accuracy: 0.6747 - tf_loss: 0.6064 - val_ie_accuracy: 0.5805 - val_ie_loss: 0.6636 - val_jp_accuracy: 0.5857 - val_jp_loss: 0.6766 - val_loss: 2.4556 - val_sn_accuracy: 0.7453 - val_sn_loss: 0.4812 - val_tf_accuracy: 0.6488 - val_tf_loss: 0.6342 - learning_rate: 5.0000e-05\n",
            "Epoch 10/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - ie_accuracy: 0.6170 - ie_loss: 0.6281 - jp_accuracy: 0.6219 - jp_loss: 0.6522 - loss: 2.2760 - sn_accuracy: 0.7717 - sn_loss: 0.3907 - tf_accuracy: 0.6767 - tf_loss: 0.6050\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 29ms/step - ie_accuracy: 0.6170 - ie_loss: 0.6281 - jp_accuracy: 0.6219 - jp_loss: 0.6522 - loss: 2.2760 - sn_accuracy: 0.7717 - sn_loss: 0.3907 - tf_accuracy: 0.6767 - tf_loss: 0.6050 - val_ie_accuracy: 0.5781 - val_ie_loss: 0.6655 - val_jp_accuracy: 0.5866 - val_jp_loss: 0.6766 - val_loss: 2.4709 - val_sn_accuracy: 0.7364 - val_sn_loss: 0.4981 - val_tf_accuracy: 0.6518 - val_tf_loss: 0.6305 - learning_rate: 5.0000e-05\n",
            "Epoch 11/12\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 29ms/step - ie_accuracy: 0.6202 - ie_loss: 0.6237 - jp_accuracy: 0.6239 - jp_loss: 0.6506 - loss: 2.2469 - sn_accuracy: 0.7843 - sn_loss: 0.3702 - tf_accuracy: 0.6791 - tf_loss: 0.6024 - val_ie_accuracy: 0.5870 - val_ie_loss: 0.6586 - val_jp_accuracy: 0.5845 - val_jp_loss: 0.6789 - val_loss: 2.4582 - val_sn_accuracy: 0.7527 - val_sn_loss: 0.4867 - val_tf_accuracy: 0.6486 - val_tf_loss: 0.6341 - learning_rate: 2.5000e-05\n",
            "Epoch 12/12\n",
            "\u001b[1m5804/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - ie_accuracy: 0.6220 - ie_loss: 0.6215 - jp_accuracy: 0.6254 - jp_loss: 0.6492 - loss: 2.2291 - sn_accuracy: 0.7939 - sn_loss: 0.3571 - tf_accuracy: 0.6790 - tf_loss: 0.6013\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\u001b[1m5805/5805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 29ms/step - ie_accuracy: 0.6220 - ie_loss: 0.6215 - jp_accuracy: 0.6254 - jp_loss: 0.6492 - loss: 2.2291 - sn_accuracy: 0.7939 - sn_loss: 0.3570 - tf_accuracy: 0.6790 - tf_loss: 0.6013 - val_ie_accuracy: 0.5860 - val_ie_loss: 0.6608 - val_jp_accuracy: 0.5869 - val_jp_loss: 0.6776 - val_loss: 2.4599 - val_sn_accuracy: 0.7559 - val_sn_loss: 0.4867 - val_tf_accuracy: 0.6482 - val_tf_loss: 0.6348 - learning_rate: 2.5000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a8452995e50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "##This is our helpers again\n",
        "MBTI16 = ['ISTJ','ISFJ','INFJ','INTJ','ISTP','ISFP','INFP','INTP',\n",
        "          'ENTJ','ENTP','ENFJ','ENFP','ESTJ','ESFJ','ESTP','ESFP']\n",
        "lab2id = {l:i for i,l in enumerate(MBTI16)}\n",
        "\n",
        "def axes_to_type_str(bits):\n",
        "    out = []\n",
        "    for b in bits:\n",
        "        out.append( ('I' if b[0]==0 else 'E') +\n",
        "                    ('S' if b[1]==0 else 'N') +\n",
        "                    ('T' if b[2]==0 else 'F') +\n",
        "                    ('J' if b[3]==0 else 'P') )\n",
        "    return out\n",
        "\n",
        "def eval_axes_model(model, Xva, y_axes_va, tag, batch_size=256):\n",
        "    ##This will predict probabilities for each head\n",
        "    ds = tf.data.Dataset.from_tensor_slices(Xva).batch(batch_size)\n",
        "    p_ie, p_sn, p_tf, p_jp = model.predict(ds, verbose=0)\n",
        "\n",
        "    ##This will set Threshold at 0.5 → bits\n",
        "    pred_bits = np.stack([\n",
        "        (p_ie[:,0] >= 0.5).astype(int),\n",
        "        (p_sn[:,0] >= 0.5).astype(int),\n",
        "        (p_tf[:,0] >= 0.5).astype(int),\n",
        "        (p_jp[:,0] >= 0.5).astype(int),\n",
        "    ], axis=1)\n",
        "\n",
        "    ##These are our per-axis metrics\n",
        "    axis_names = [\"IE\",\"SN\",\"TF\",\"JP\"]\n",
        "    print(f\"\\n--- {tag}: Per-axis metrics ---\")\n",
        "    for j,name in enumerate(axis_names):\n",
        "        acc = accuracy_score(y_axes_va[:,j], pred_bits[:,j])\n",
        "        f1  = f1_score(y_axes_va[:,j], pred_bits[:,j], average=\"macro\")\n",
        "        print(f\"{name}: acc={acc:.3f}  macroF1={f1:.3f}\")\n",
        "\n",
        "    ##This will reconstruct 16-type predictions\n",
        "    true_types = axes_to_type_str(y_axes_va)\n",
        "    pred_types = axes_to_type_str(pred_bits)\n",
        "    y_true = np.array([lab2id[t] for t in true_types])\n",
        "    y_pred = np.array([lab2id[t] for t in pred_types])\n",
        "\n",
        "    acc16 = accuracy_score(y_true, y_pred)\n",
        "    f116  = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    print(f\"\\n--- {tag}: 16-way metrics ---\")\n",
        "    print(\"Accuracy:\", acc16)\n",
        "    print(\"Macro-F1:\", f116)\n",
        "    print(classification_report(y_true, y_pred, target_names=MBTI16, digits=3))\n",
        "\n",
        "    return acc16, f116\n",
        "\n",
        "def cross_domain_drop(within_f1, cross_f1):\n",
        "    return float(within_f1 - cross_f1)\n",
        "\n",
        "##This will run all the evals\n",
        "BATCH_EVAL = 256 if tf.config.list_physical_devices('GPU') else 64\n",
        "\n",
        "##This is 1) Within-domain\n",
        "acc_k_within, f1_k_within = eval_axes_model(m_k, Xk_va, yk_axes_va, \"Kaggle → Kaggle (val)\", BATCH_EVAL)\n",
        "acc_r_within, f1_r_within = eval_axes_model(m_r, Xr_va, yr_axes_va, \"Reddit → Reddit (val)\", BATCH_EVAL)\n",
        "\n",
        "##This is 2) Cross-domain\n",
        "acc_k_cross,  f1_k_cross  = eval_axes_model(m_k, Xr_va, yr_axes_va, \"Kaggle → Reddit (cross)\", BATCH_EVAL)\n",
        "acc_r_cross,  f1_r_cross  = eval_axes_model(m_r, Xk_va, yk_axes_va, \"Reddit → Kaggle (cross)\", BATCH_EVAL)\n",
        "\n",
        "##This is 3) F1 summary\n",
        "print(\"\\n=== Cross-domain drop (ΔF1 = within − cross) ===\")\n",
        "print(f\"Kaggle-trained ΔF1: {cross_domain_drop(f1_k_within, f1_k_cross):.4f}\")\n",
        "print(f\"Reddit-trained ΔF1: {cross_domain_drop(f1_r_within, f1_r_cross):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb0cTqhMbyqw",
        "outputId": "bc3cd906-7c95-4d45-fe5a-7f0b19d5120f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Kaggle → Kaggle (val): Per-axis metrics ---\n",
            "IE: acc=0.600  macroF1=0.531\n",
            "SN: acc=0.638  macroF1=0.505\n",
            "TF: acc=0.599  macroF1=0.597\n",
            "JP: acc=0.541  macroF1=0.536\n",
            "\n",
            "--- Kaggle → Kaggle (val): 16-way metrics ---\n",
            "Accuracy: 0.13328628443492652\n",
            "Macro-F1: 0.0894238515343283\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.048     0.105     0.066       963\n",
            "        ISFJ      0.033     0.077     0.046       789\n",
            "        INFJ      0.231     0.198     0.213      6999\n",
            "        INTJ      0.185     0.158     0.171      5113\n",
            "        ISTP      0.073     0.101     0.085      1598\n",
            "        ISFP      0.048     0.077     0.059      1246\n",
            "        INFP      0.296     0.141     0.191      8696\n",
            "        INTP      0.232     0.102     0.142      6144\n",
            "        ENTJ      0.051     0.092     0.066      1099\n",
            "        ENTP      0.127     0.104     0.114      3302\n",
            "        ENFJ      0.038     0.084     0.052       910\n",
            "        ENFP      0.130     0.124     0.127      3208\n",
            "        ESTJ      0.011     0.069     0.019       188\n",
            "        ESFJ      0.014     0.060     0.022       199\n",
            "        ESTP      0.022     0.116     0.037       424\n",
            "        ESFP      0.012     0.093     0.021       214\n",
            "\n",
            "    accuracy                          0.133     41092\n",
            "   macro avg      0.097     0.106     0.089     41092\n",
            "weighted avg      0.189     0.133     0.149     41092\n",
            "\n",
            "\n",
            "--- Reddit → Reddit (val): Per-axis metrics ---\n",
            "IE: acc=0.592  macroF1=0.545\n",
            "SN: acc=0.740  macroF1=0.509\n",
            "TF: acc=0.650  macroF1=0.620\n",
            "JP: acc=0.581  macroF1=0.570\n",
            "\n",
            "--- Reddit → Reddit (val): 16-way metrics ---\n",
            "Accuracy: 0.18490703167585246\n",
            "Macro-F1: 0.09878951149239515\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.020     0.044     0.027      1659\n",
            "        ISFJ      0.010     0.031     0.015       700\n",
            "        INFJ      0.226     0.243     0.234     19468\n",
            "        INTJ      0.334     0.262     0.294     35804\n",
            "        ISTP      0.057     0.073     0.064      5006\n",
            "        ISFP      0.029     0.051     0.037      1134\n",
            "        INFP      0.216     0.101     0.138     17699\n",
            "        INTP      0.396     0.174     0.242     45224\n",
            "        ENTJ      0.058     0.080     0.067      4364\n",
            "        ENTP      0.207     0.188     0.197     19434\n",
            "        ENFJ      0.032     0.149     0.053      2094\n",
            "        ENFP      0.110     0.156     0.129      9784\n",
            "        ESTJ      0.012     0.031     0.017       448\n",
            "        ESFJ      0.005     0.038     0.008       265\n",
            "        ESTP      0.017     0.193     0.031      1279\n",
            "        ESFP      0.015     0.162     0.027       748\n",
            "\n",
            "    accuracy                          0.185    165110\n",
            "   macro avg      0.109     0.124     0.099    165110\n",
            "weighted avg      0.266     0.185     0.209    165110\n",
            "\n",
            "\n",
            "--- Kaggle → Reddit (cross): Per-axis metrics ---\n",
            "IE: acc=0.620  macroF1=0.532\n",
            "SN: acc=0.662  macroF1=0.465\n",
            "TF: acc=0.599  macroF1=0.577\n",
            "JP: acc=0.542  macroF1=0.536\n",
            "\n",
            "--- Kaggle → Reddit (cross): 16-way metrics ---\n",
            "Accuracy: 0.1379746835443038\n",
            "Macro-F1: 0.07555369514476917\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.013     0.060     0.021      1659\n",
            "        ISFJ      0.008     0.057     0.014       700\n",
            "        INFJ      0.185     0.255     0.214     19468\n",
            "        INTJ      0.285     0.175     0.216     35804\n",
            "        ISTP      0.042     0.098     0.059      5006\n",
            "        ISFP      0.008     0.049     0.014      1134\n",
            "        INFP      0.170     0.151     0.160     17699\n",
            "        INTP      0.350     0.106     0.163     45224\n",
            "        ENTJ      0.041     0.069     0.051      4364\n",
            "        ENTP      0.164     0.099     0.123     19434\n",
            "        ENFJ      0.019     0.048     0.027      2094\n",
            "        ENFP      0.099     0.089     0.094      9784\n",
            "        ESTJ      0.004     0.042     0.008       448\n",
            "        ESFJ      0.003     0.023     0.005       265\n",
            "        ESTP      0.013     0.105     0.023      1279\n",
            "        ESFP      0.010     0.067     0.017       748\n",
            "\n",
            "    accuracy                          0.138    165110\n",
            "   macro avg      0.088     0.093     0.076    165110\n",
            "weighted avg      0.226     0.138     0.158    165110\n",
            "\n",
            "\n",
            "--- Reddit → Kaggle (cross): Per-axis metrics ---\n",
            "IE: acc=0.564  macroF1=0.513\n",
            "SN: acc=0.702  macroF1=0.516\n",
            "TF: acc=0.593  macroF1=0.593\n",
            "JP: acc=0.537  macroF1=0.527\n",
            "\n",
            "--- Reddit → Kaggle (cross): 16-way metrics ---\n",
            "Accuracy: 0.13189915311982867\n",
            "Macro-F1: 0.0816387359837833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.038     0.042     0.040       963\n",
            "        ISFJ      0.043     0.047     0.045       789\n",
            "        INFJ      0.217     0.179     0.196      6999\n",
            "        INTJ      0.176     0.207     0.190      5113\n",
            "        ISTP      0.069     0.057     0.062      1598\n",
            "        ISFP      0.038     0.020     0.026      1246\n",
            "        INFP      0.302     0.097     0.147      8696\n",
            "        INTP      0.210     0.151     0.175      6144\n",
            "        ENTJ      0.053     0.066     0.059      1099\n",
            "        ENTP      0.119     0.138     0.128      3302\n",
            "        ENFJ      0.035     0.108     0.052       910\n",
            "        ENFP      0.108     0.139     0.121      3208\n",
            "        ESTJ      0.003     0.005     0.004       188\n",
            "        ESFJ      0.010     0.035     0.015       199\n",
            "        ESTP      0.017     0.108     0.029       424\n",
            "        ESFP      0.009     0.093     0.016       214\n",
            "\n",
            "    accuracy                          0.132     41092\n",
            "   macro avg      0.090     0.093     0.082     41092\n",
            "weighted avg      0.180     0.132     0.142     41092\n",
            "\n",
            "\n",
            "=== Cross-domain drop (ΔF1 = within − cross) ===\n",
            "Kaggle-trained ΔF1: 0.0139\n",
            "Reddit-trained ΔF1: 0.0172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THIRD PART**"
      ],
      "metadata": {
        "id": "aomVdTOicdMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "\n",
        "##MBTI helpers again\n",
        "MBTI16 = [\n",
        "    'ISTJ','ISFJ','INFJ','INTJ',\n",
        "    'ISTP','ISFP','INFP','INTP',\n",
        "    'ENTJ','ENTP','ENFJ','ENFP',\n",
        "    'ESTJ','ESFJ','ESTP','ESFP'\n",
        "]\n",
        "lab2id = {l:i for i,l in enumerate(MBTI16)}\n",
        "\n",
        "##This will map each 16-type to axis bits [IE,SN,TF,JP] where bit=1 means E/N/F/P\n",
        "def type_to_bits(t):\n",
        "    return np.array([\n",
        "        1 if t[0]=='E' else 0,\n",
        "        1 if t[1]=='N' else 0,\n",
        "        1 if t[2]=='F' else 0,\n",
        "        1 if t[3]=='P' else 0,\n",
        "    ], dtype=int)\n",
        "\n",
        "TYPE_BITS = np.stack([type_to_bits(t) for t in MBTI16], axis=0)\n",
        "\n",
        "BIT_WEIGHTS = np.array([8, 4, 2, 1], dtype=np.int8)\n",
        "\n",
        "def axes_to_type_str(bits):\n",
        "    out = []\n",
        "    for b in bits:\n",
        "        out.append(\n",
        "            ('I' if b[0]==0 else 'E') +\n",
        "            ('S' if b[1]==0 else 'N') +\n",
        "            ('T' if b[2]==0 else 'F') +\n",
        "            ('J' if b[3]==0 else 'P')\n",
        "        )\n",
        "    return out\n",
        "\n",
        "##This will predict axis probablity\n",
        "def predict_axis_probs(model, X, batch_size=512):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(X).batch(batch_size)\n",
        "    p_ie, p_sn, p_tf, p_jp = model.predict(ds, verbose=0)\n",
        "    ##This will squeeze to shape (N,) each\n",
        "    return p_ie[:, 0], p_sn[:, 0], p_tf[:, 0], p_jp[:, 0]\n",
        "\n",
        "##This will implement MAP decoding over 16 types\n",
        "def map_decode(p_ie, p_sn, p_tf, p_jp, priors=None):\n",
        "    \"\"\"\n",
        "    Compute P(type) = Π_j [p_j if bit==1 else (1-p_j)] * prior[type].\n",
        "    p_* are shape (N,), priors is shape (16,) or None for uniform.\n",
        "    Returns: y_pred_type_ids (N,)\n",
        "    \"\"\"\n",
        "    N = p_ie.shape[0]\n",
        "    P = np.stack([p_ie, p_sn, p_tf, p_jp], axis=1)\n",
        "\n",
        "    like = np.ones((N, 16), dtype=np.float64)\n",
        "    for j in range(4):\n",
        "        pj = P[:, j][:, None]\n",
        "        bj = TYPE_BITS[None, :, j]\n",
        "        like *= np.where(bj == 1, pj, 1.0 - pj)\n",
        "\n",
        "    if priors is None:\n",
        "        post = like\n",
        "    else:\n",
        "        priors = np.asarray(priors, dtype=np.float64)[None, :]\n",
        "        post = like * priors\n",
        "\n",
        "    y_pred = np.argmax(post, axis=1).astype(int)\n",
        "    return y_pred\n",
        "\n",
        "def eval_map(model, X, y_axes_true, tag, priors=None, batch_size=512):\n",
        "    ##These are true 16-type ids from axis labels\n",
        "    y_true_types = axes_to_type_str(y_axes_true.astype(int))\n",
        "    y_true = np.array([lab2id[t] for t in y_true_types])\n",
        "\n",
        "    ##This will predict axis probs\n",
        "    p_ie, p_sn, p_tf, p_jp = predict_axis_probs(model, X, batch_size=batch_size)\n",
        "\n",
        "    ##This is the MAP decoder\n",
        "    y_pred = map_decode(p_ie, p_sn, p_tf, p_jp, priors=priors)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    print(f\"\\n--- {tag} (MAP decode) ---\")\n",
        "    if priors is None:\n",
        "        print(\"Priors: uniform\")\n",
        "    else:\n",
        "        print(\"Priors: train distribution\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Macro-F1:\", f1)\n",
        "    print(classification_report(y_true, y_pred, target_names=MBTI16, digits=3))\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "def estimate_priors_from_axes(y_axes):\n",
        "    \"\"\"\n",
        "    Estimate 16-type priors from axis labels by counting each 16-type code derived from axes.\n",
        "    \"\"\"\n",
        "    bits = y_axes.astype(int)\n",
        "    types = axes_to_type_str(bits)\n",
        "    ids = np.array([lab2id[t] for t in types])\n",
        "    counts = np.bincount(ids, minlength=16).astype(np.float64)\n",
        "    priors = counts / counts.sum()\n",
        "    ##This will set a small floor to avoid exact zero\n",
        "    priors = np.clip(priors, 1e-6, None)\n",
        "    priors /= priors.sum()\n",
        "    return priors\n",
        "\n",
        "##This is a fast threshold search over axes\n",
        "def grid_search_thresholds_for_16way_fast(p_ie, p_sn, p_tf, p_jp, y_axes_true,\n",
        "                                          t_values=None):\n",
        "    \"\"\"\n",
        "    Faster threshold search:\n",
        "      - operates in integer MBTI IDs (0..15), no strings in the inner loop\n",
        "      - uses a small grid of thresholds per axis (default: 5 values in [0.35, 0.65])\n",
        "    \"\"\"\n",
        "    if t_values is None:\n",
        "        t_values = np.linspace(0.35, 0.65, 5)\n",
        "\n",
        "    ##This will get true type IDs from true axis bits\n",
        "    y_true_bits = y_axes_true.astype(int)\n",
        "    y_true_ids  = y_true_bits @ BIT_WEIGHTS\n",
        "\n",
        "    best = {\"f1\": -1.0, \"acc\": 0.0, \"thr\": (0.5, 0.5, 0.5, 0.5)}\n",
        "\n",
        "    for t_ie, t_sn, t_tf, t_jp in product(t_values, repeat=4):\n",
        "        ##This sets threshold each axis once for this combo\n",
        "        ie_bits = (p_ie >= t_ie).astype(int)\n",
        "        sn_bits = (p_sn >= t_sn).astype(int)\n",
        "        tf_bits = (p_tf >= t_tf).astype(int)\n",
        "        jp_bits = (p_jp >= t_jp).astype(int)\n",
        "\n",
        "        ##This will stack into (N,4) and map to 0..15 IDs\n",
        "        pred_bits = np.stack([ie_bits, sn_bits, tf_bits, jp_bits], axis=1)\n",
        "        y_pred_ids = pred_bits @ BIT_WEIGHTS\n",
        "\n",
        "        f1  = f1_score(y_true_ids, y_pred_ids, average=\"macro\")\n",
        "        if f1 > best[\"f1\"]:\n",
        "            acc = accuracy_score(y_true_ids, y_pred_ids)\n",
        "            best[\"f1\"]  = float(f1)\n",
        "            best[\"acc\"] = float(acc)\n",
        "            best[\"thr\"] = (float(t_ie), float(t_sn), float(t_tf), float(t_jp))\n",
        "\n",
        "    return best\n",
        "\n",
        "\n",
        "##This will run the improved decoders\n",
        "print(\">>> MAP decoding on Kaggle val\")\n",
        "priors_k = estimate_priors_from_axes(yk_axes_va)\n",
        "acc_k_map_u, f1_k_map_u = eval_map(m_k, Xk_va, yk_axes_va, \"Kaggle→Kaggle\", priors=None)\n",
        "acc_k_map_p, f1_k_map_p = eval_map(m_k, Xk_va, yk_axes_va, \"Kaggle→Kaggle\", priors=priors_k)\n",
        "\n",
        "print(\"\\n>>> MAP decoding on Reddit val\")\n",
        "priors_r = estimate_priors_from_axes(yr_axes_va)\n",
        "acc_r_map_u, f1_r_map_u = eval_map(m_r, Xr_va, yr_axes_va, \"Reddit→Reddit\", priors=None)\n",
        "acc_r_map_p, f1_r_map_p = eval_map(m_r, Xr_va, yr_axes_va, \"Reddit→Reddit\", priors=priors_r)\n",
        "\n",
        "##This will tune the threshold with fast search\n",
        "print(\"\\n>>> Threshold search (Kaggle val)\")\n",
        "p_ie_k, p_sn_k, p_tf_k, p_jp_k = predict_axis_probs(m_k, Xk_va)\n",
        "best_k = grid_search_thresholds_for_16way_fast(\n",
        "    p_ie_k, p_sn_k, p_tf_k, p_jp_k, yk_axes_va\n",
        ")\n",
        "print(\"Best thresholds (ie,sn,tf,jp):\", best_k[\"thr\"],\n",
        "      \"  acc:\", best_k[\"acc\"], \"  macroF1:\", best_k[\"f1\"])\n",
        "\n",
        "print(\"\\n>>> Threshold search (Reddit val)\")\n",
        "p_ie_r, p_sn_r, p_tf_r, p_jp_r = predict_axis_probs(m_r, Xr_va)\n",
        "best_r = grid_search_thresholds_for_16way_fast(\n",
        "    p_ie_r, p_sn_r, p_tf_r, p_jp_r, yr_axes_va\n",
        ")\n",
        "print(\"Best thresholds (ie,sn,tf,jp):\", best_r[\"thr\"],\n",
        "      \"  acc:\", best_r[\"acc\"], \"  macroF1:\", best_r[\"f1\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6QlTc5Vcg20",
        "outputId": "fbcba9aa-4fef-4bb8-e056-70273be7fc58"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> MAP decoding on Kaggle val\n",
            "\n",
            "--- Kaggle→Kaggle (MAP decode) ---\n",
            "Priors: uniform\n",
            "Accuracy: 0.13328628443492652\n",
            "Macro-F1: 0.0894238515343283\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.048     0.105     0.066       963\n",
            "        ISFJ      0.033     0.077     0.046       789\n",
            "        INFJ      0.231     0.198     0.213      6999\n",
            "        INTJ      0.185     0.158     0.171      5113\n",
            "        ISTP      0.073     0.101     0.085      1598\n",
            "        ISFP      0.048     0.077     0.059      1246\n",
            "        INFP      0.296     0.141     0.191      8696\n",
            "        INTP      0.232     0.102     0.142      6144\n",
            "        ENTJ      0.051     0.092     0.066      1099\n",
            "        ENTP      0.127     0.104     0.114      3302\n",
            "        ENFJ      0.038     0.084     0.052       910\n",
            "        ENFP      0.130     0.124     0.127      3208\n",
            "        ESTJ      0.011     0.069     0.019       188\n",
            "        ESFJ      0.014     0.060     0.022       199\n",
            "        ESTP      0.022     0.116     0.037       424\n",
            "        ESFP      0.012     0.093     0.021       214\n",
            "\n",
            "    accuracy                          0.133     41092\n",
            "   macro avg      0.097     0.106     0.089     41092\n",
            "weighted avg      0.189     0.133     0.149     41092\n",
            "\n",
            "\n",
            "--- Kaggle→Kaggle (MAP decode) ---\n",
            "Priors: train distribution\n",
            "Accuracy: 0.223936532658425\n",
            "Macro-F1: 0.08253993204899057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.198     0.020     0.036       963\n",
            "        ISFJ      0.111     0.001     0.003       789\n",
            "        INFJ      0.221     0.279     0.247      6999\n",
            "        INTJ      0.191     0.173     0.182      5113\n",
            "        ISTP      0.087     0.036     0.051      1598\n",
            "        ISFP      0.141     0.008     0.015      1246\n",
            "        INFP      0.253     0.524     0.341      8696\n",
            "        INTP      0.219     0.206     0.213      6144\n",
            "        ENTJ      0.000     0.000     0.000      1099\n",
            "        ENTP      0.145     0.093     0.113      3302\n",
            "        ENFJ      0.000     0.000     0.000       910\n",
            "        ENFP      0.170     0.045     0.071      3208\n",
            "        ESTJ      0.750     0.016     0.031       188\n",
            "        ESFJ      0.000     0.000     0.000       199\n",
            "        ESTP      0.000     0.000     0.000       424\n",
            "        ESFP      1.000     0.009     0.019       214\n",
            "\n",
            "    accuracy                          0.224     41092\n",
            "   macro avg      0.218     0.088     0.083     41092\n",
            "weighted avg      0.196     0.224     0.187     41092\n",
            "\n",
            "\n",
            ">>> MAP decoding on Reddit val\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Reddit→Reddit (MAP decode) ---\n",
            "Priors: uniform\n",
            "Accuracy: 0.18490703167585246\n",
            "Macro-F1: 0.09878951149239515\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.020     0.044     0.027      1659\n",
            "        ISFJ      0.010     0.031     0.015       700\n",
            "        INFJ      0.226     0.243     0.234     19468\n",
            "        INTJ      0.334     0.262     0.294     35804\n",
            "        ISTP      0.057     0.073     0.064      5006\n",
            "        ISFP      0.029     0.051     0.037      1134\n",
            "        INFP      0.216     0.101     0.138     17699\n",
            "        INTP      0.396     0.174     0.242     45224\n",
            "        ENTJ      0.058     0.080     0.067      4364\n",
            "        ENTP      0.207     0.188     0.197     19434\n",
            "        ENFJ      0.032     0.149     0.053      2094\n",
            "        ENFP      0.110     0.156     0.129      9784\n",
            "        ESTJ      0.012     0.031     0.017       448\n",
            "        ESFJ      0.005     0.038     0.008       265\n",
            "        ESTP      0.017     0.193     0.031      1279\n",
            "        ESFP      0.015     0.162     0.027       748\n",
            "\n",
            "    accuracy                          0.185    165110\n",
            "   macro avg      0.109     0.124     0.099    165110\n",
            "weighted avg      0.266     0.185     0.209    165110\n",
            "\n",
            "\n",
            "--- Reddit→Reddit (MAP decode) ---\n",
            "Priors: train distribution\n",
            "Accuracy: 0.30629277451396036\n",
            "Macro-F1: 0.08944273640127648\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ISTJ      0.500     0.001     0.002      1659\n",
            "        ISFJ      0.000     0.000     0.000       700\n",
            "        INFJ      0.260     0.250     0.255     19468\n",
            "        INTJ      0.321     0.301     0.311     35804\n",
            "        ISTP      0.129     0.029     0.047      5006\n",
            "        ISFP      0.400     0.002     0.004      1134\n",
            "        INFP      0.305     0.074     0.119     17699\n",
            "        INTP      0.318     0.676     0.433     45224\n",
            "        ENTJ      0.727     0.002     0.004      4364\n",
            "        ENTP      0.284     0.123     0.171     19434\n",
            "        ENFJ      0.000     0.000     0.000      2094\n",
            "        ENFP      0.174     0.052     0.081      9784\n",
            "        ESTJ      0.000     0.000     0.000       448\n",
            "        ESFJ      0.000     0.000     0.000       265\n",
            "        ESTP      0.286     0.002     0.003      1279\n",
            "        ESFP      0.111     0.001     0.003       748\n",
            "\n",
            "    accuracy                          0.306    165110\n",
            "   macro avg      0.238     0.094     0.089    165110\n",
            "weighted avg      0.297     0.306     0.255    165110\n",
            "\n",
            "\n",
            ">>> Threshold search (Kaggle val)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best thresholds (ie,sn,tf,jp): (0.65, 0.35, 0.5, 0.5)   acc: 0.19354132191180765   macroF1: 0.0982407568673606\n",
            "\n",
            ">>> Threshold search (Reddit val)\n",
            "Best thresholds (ie,sn,tf,jp): (0.575, 0.35, 0.575, 0.5)   acc: 0.25383077948034644   macroF1: 0.11266795073323638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FOURTH PART**"
      ],
      "metadata": {
        "id": "IBs9zUvudaZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def focal_bce(gamma=2.0, alpha=0.5):\n",
        "    def loss(y_true, y_pred):\n",
        "        ##This will clip predictions\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n",
        "\n",
        "        ##pt = p if correct else 1-p\n",
        "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "\n",
        "        ##alpha balancing\n",
        "        w = alpha * y_true + (1 - alpha) * (1 - y_true)\n",
        "\n",
        "        ##focal loss formula\n",
        "        return tf.reduce_mean(-w * tf.pow(1 - pt, gamma) * tf.math.log(pt))\n",
        "    return loss\n",
        "\n",
        "def make_cnn_axes_plus_softmax(vocab_size, seq_len, emb_dim=96, filters=(192,192,192), drop=0.25, num_types=16, label_smooth=0.05):\n",
        "    inp = L.Input(shape=(seq_len,), dtype=\"int32\", name=\"input_layer\")\n",
        "    x = L.Embedding(vocab_size, emb_dim, name=\"embedding\")(inp)\n",
        "    c3 = L.Conv1D(filters[0], 3, padding=\"valid\", activation=\"relu\")(x)\n",
        "    c4 = L.Conv1D(filters[1], 4, padding=\"valid\", activation=\"relu\")(x)\n",
        "    c5 = L.Conv1D(filters[2], 5, padding=\"valid\", activation=\"relu\")(x)\n",
        "    p = L.Concatenate()([L.GlobalMaxPooling1D()(c3),\n",
        "                         L.GlobalMaxPooling1D()(c4),\n",
        "                         L.GlobalMaxPooling1D()(c5)])\n",
        "    p = L.Dropout(drop)(p)\n",
        "    h = L.Dense(256, activation=\"relu\")(p)\n",
        "    h = L.Dropout(drop)(h)\n",
        "\n",
        "    ##This will set the 4 axis heads\n",
        "    ie = L.Dense(1, activation=\"sigmoid\", name=\"ie\")(h)\n",
        "    sn = L.Dense(1, activation=\"sigmoid\", name=\"sn\")(h)\n",
        "    tfh = L.Dense(1, activation=\"sigmoid\", name=\"tf\")(h)\n",
        "    jp = L.Dense(1, activation=\"sigmoid\", name=\"jp\")(h)\n",
        "\n",
        "    ##This will create the 16-way softmax head\n",
        "    typelogits = L.Dense(num_types, activation=None, name=\"type_logits\")(h)\n",
        "    typesmx    = L.Activation(\"softmax\", name=\"type\")(typelogits)\n",
        "\n",
        "    model = keras.Model(inp, [ie, sn, tfh, jp, typesmx], name=\"axes_plus_softmax\")\n",
        "\n",
        "    ##This will capture losses\n",
        "    losses = {\n",
        "        \"ie\": focal_bce(gamma=2.0, alpha=0.5),\n",
        "        \"sn\": focal_bce(gamma=2.0, alpha=0.5),\n",
        "        \"tf\": focal_bce(gamma=2.0, alpha=0.5),\n",
        "        \"jp\": focal_bce(gamma=2.0, alpha=0.5),\n",
        "        \"type\": keras.losses.CategoricalCrossentropy(label_smoothing=label_smooth),\n",
        "    }\n",
        "    ##This will set the balance so softmax head gets a decent weight\n",
        "    loss_w = {\"ie\":1.0, \"sn\":1.0, \"tf\":1.0, \"jp\":1.0, \"type\":2.0}\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(3e-4),\n",
        "        loss=losses,\n",
        "        loss_weights=loss_w,\n",
        "        metrics={\n",
        "            \"ie\":[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
        "            \"sn\":[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
        "            \"tf\":[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
        "            \"jp\":[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
        "            \"type\":[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
        "        }\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SbIEO0mBddDG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "MBTI16 = ['ISTJ','ISFJ','INFJ','INTJ','ISTP','ISFP','INFP','INTP',\n",
        "          'ENTJ','ENTP','ENFJ','ENFP','ESTJ','ESFJ','ESTP','ESFP']\n",
        "lab2id = {l:i for i,l in enumerate(MBTI16)}\n",
        "\n",
        "def axes_to_type_str(bits):\n",
        "    out=[]\n",
        "    for b in bits:\n",
        "        out.append(('I' if b[0]==0 else 'E')+\n",
        "                   ('S' if b[1]==0 else 'N')+\n",
        "                   ('T' if b[2]==0 else 'F')+\n",
        "                   ('J' if b[3]==0 else 'P'))\n",
        "    return out\n",
        "\n",
        "def onehot(ids, K=16):\n",
        "    y = np.zeros((len(ids), K), dtype=np.float32)\n",
        "    y[np.arange(len(ids)), ids] = 1.0\n",
        "    return y\n",
        "\n",
        "##This will build y_type for each split I already created\n",
        "def make_y_type_from_axes(y_axes):\n",
        "    t = axes_to_type_str(y_axes.astype(int))\n",
        "    ids = np.array([lab2id[s] for s in t], dtype=int)\n",
        "    return onehot(ids, K=16)\n",
        "\n",
        "##This is for Kaggle val/train parts I already have\n",
        "y_k_tr_type = make_y_type_from_axes(yk_axes[ds_k_tr.unbatch().map(lambda x,y,w: y['ie']).cardinality().numpy() * 0:0])  # ignore, see below\n"
      ],
      "metadata": {
        "id": "2GK91m70dlgo"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def make_axis_ds_with_weights(X, y_axes, wdicts, val_frac=0.10, batch=512, seed=42):\n",
        "    \"\"\"\n",
        "    Build tf.data datasets for the 4 binary MBTI axes *and* return the\n",
        "    train/val indices so you can derive 16-way one-hot labels later.\n",
        "\n",
        "    Returns:\n",
        "        ds_tr, ds_va, (Xva, yva), (idx_tr, idx_va)\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "\n",
        "    n_va = int(n * val_frac)\n",
        "    idx_va = idx[:n_va]\n",
        "    idx_tr = idx[n_va:]\n",
        "\n",
        "    Xtr, Xva = X[idx_tr], X[idx_va]\n",
        "    ytr, yva = y_axes[idx_tr], y_axes[idx_va]\n",
        "\n",
        "    ##This will build dicts for the 4 heads\n",
        "    def to_y_dict(Y):\n",
        "        Y = Y.astype(np.float32)\n",
        "        return {\n",
        "            \"ie\": Y[:, 0],\n",
        "            \"sn\": Y[:, 1],\n",
        "            \"tf\": Y[:, 2],\n",
        "            \"jp\": Y[:, 3],\n",
        "        }\n",
        "    ytr_dict = to_y_dict(ytr)\n",
        "    yva_dict = to_y_dict(yva)\n",
        "\n",
        "    ##This will set sample weights for each axis head (tuple order must match model outputs)\n",
        "    ##Here wdicts is a list/tuple of 4 dicts like {0: w_for_zero, 1: w_for_one}\n",
        "    Wtr = np.empty_like(ytr, dtype=np.float32)\n",
        "    for j in range(4):\n",
        "        Wtr[:, j] = np.where(ytr[:, j] == 1, wdicts[j][1], wdicts[j][0])\n",
        "\n",
        "    sw_tr = (Wtr[:, 0], Wtr[:, 1], Wtr[:, 2], Wtr[:, 3])\n",
        "    sw_va = (\n",
        "        np.ones(len(yva), np.float32),\n",
        "        np.ones(len(yva), np.float32),\n",
        "        np.ones(len(yva), np.float32),\n",
        "        np.ones(len(yva), np.float32),\n",
        "    )\n",
        "\n",
        "    ds_tr = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices((Xtr, ytr_dict, sw_tr))\n",
        "          .shuffle(10000, seed=seed)\n",
        "          .batch(batch)\n",
        "          .prefetch(AUTOTUNE)\n",
        "    )\n",
        "    ds_va = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices((Xva, yva_dict, sw_va))\n",
        "          .batch(batch)\n",
        "          .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    ##This will also return (Xva, yva) for metrics, and (idx_tr, idx_va) for building 16 way labels\n",
        "    return ds_tr, ds_va, (Xva, yva), (idx_tr, idx_va)\n"
      ],
      "metadata": {
        "id": "_vtHrwcyByn4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "##This will Train/val split (Kaggle)\n",
        "rng = np.random.RandomState(42)\n",
        "n_k = len(Xk)\n",
        "idx = np.arange(n_k)\n",
        "rng.shuffle(idx)\n",
        "\n",
        "val_frac = 0.10\n",
        "n_va = int(n_k * val_frac)\n",
        "idx_va = idx[:n_va]\n",
        "idx_tr = idx[n_va:]\n",
        "\n",
        "Xk_tr, Xk_va = Xk[idx_tr], Xk[idx_va]\n",
        "yk_axes_tr, yk_axes_va = yk_axes[idx_tr], yk_axes[idx_va]\n",
        "\n",
        "print(\"Train size:\", Xk_tr.shape[0], \" Val size:\", Xk_va.shape[0])\n",
        "\n",
        "##This will build 16-way one-hot labels from axes\n",
        "y_type_tr = make_y_type_from_axes(yk_axes_tr)\n",
        "y_type_va = make_y_type_from_axes(yk_axes_va)\n",
        "\n",
        "##This will build label dicts for all 5 heads\n",
        "y_tr_dict = {\n",
        "    \"ie\":   yk_axes_tr[:, 0].astype(np.float32),\n",
        "    \"sn\":   yk_axes_tr[:, 1].astype(np.float32),\n",
        "    \"tf\":   yk_axes_tr[:, 2].astype(np.float32),\n",
        "    \"jp\":   yk_axes_tr[:, 3].astype(np.float32),\n",
        "    \"type\": y_type_tr,\n",
        "}\n",
        "\n",
        "y_va_dict = {\n",
        "    \"ie\":   yk_axes_va[:, 0].astype(np.float32),\n",
        "    \"sn\":   yk_axes_va[:, 1].astype(np.float32),\n",
        "    \"tf\":   yk_axes_va[:, 2].astype(np.float32),\n",
        "    \"jp\":   yk_axes_va[:, 3].astype(np.float32),\n",
        "    \"type\": y_type_va,\n",
        "}\n",
        "\n",
        "##This will build the multi task model\n",
        "m_k_multi = make_cnn_axes_plus_softmax(\n",
        "    vocab_size=VOCAB_SZ,\n",
        "    seq_len=MAX_LEN,\n",
        "    emb_dim=96,\n",
        "    filters=(192, 192, 192),\n",
        "    drop=0.25,\n",
        "    num_types=16,\n",
        "    label_smooth=0.05\n",
        ")\n",
        "\n",
        "m_k_multi.summary()\n",
        "\n",
        "##This will train (test run, few epochs, NO sample_weight)\n",
        "history_multi_k = m_k_multi.fit(\n",
        "    Xk_tr,\n",
        "    y_tr_dict,\n",
        "    validation_data=(Xk_va, y_va_dict),\n",
        "    batch_size=256,\n",
        "    epochs=3,\n",
        "    callbacks=cb,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "##This will do a 16 type evaluation from type head\n",
        "##The model outputs should be [ie, sn, tf, jp, type]\n",
        "outputs_va = m_k_multi.predict(Xk_va, batch_size=256, verbose=0)\n",
        "probs_type_va = outputs_va[-1]\n",
        "\n",
        "y_pred_ids = probs_type_va.argmax(axis=1)\n",
        "\n",
        "##This will be our true ids from axes\n",
        "BIT_WEIGHTS = np.array([8, 4, 2, 1], dtype=int)\n",
        "y_true_ids = (yk_axes_va.astype(int) @ BIT_WEIGHTS)\n",
        "\n",
        "print(\"\\nMulti-task type head on Kaggle val:\")\n",
        "print(\"  accuracy:\", accuracy_score(y_true_ids, y_pred_ids))\n",
        "print(\"  macro-F1:\", f1_score(y_true_ids, y_pred_ids, average=\"macro\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VYDoEY8CE00q",
        "outputId": "2eb283f3-6edc-435b-abaa-067029068c78"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 369824  Val size: 41091\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"axes_plus_softmax\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"axes_plus_softmax\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │  \u001b[38;5;34m1,920,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_36 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m192\u001b[0m)  │     \u001b[38;5;34m55,488\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_37 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m192\u001b[0m)  │     \u001b[38;5;34m73,920\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_38 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m192\u001b[0m)  │     \u001b[38;5;34m92,352\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_12      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
              "│                     │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate_12[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m147,712\u001b[0m │ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ type_logits (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m4,112\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ie (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sn (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tf (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ jp (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ type (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ type_logits[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">55,488</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,920</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">92,352</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_12      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
              "│                     │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,712</span> │ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ type_logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,112</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ie (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tf (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ jp (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ type (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ type_logits[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,294,612\u001b[0m (8.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,294,612</span> (8.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,294,612\u001b[0m (8.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,294,612</span> (8.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 35ms/step - ie_acc: 0.7589 - ie_loss: 0.0719 - jp_acc: 0.5740 - jp_loss: 0.0863 - loss: 5.0246 - sn_acc: 0.8621 - sn_loss: 0.0553 - tf_acc: 0.5264 - tf_loss: 0.0875 - type_acc: 0.2043 - type_loss: 2.3618 - val_ie_acc: 0.7678 - val_ie_loss: 0.0690 - val_jp_acc: 0.6045 - val_jp_loss: 0.0841 - val_loss: 4.8693 - val_sn_acc: 0.8659 - val_sn_loss: 0.0520 - val_tf_acc: 0.6019 - val_tf_loss: 0.0828 - val_type_acc: 0.2293 - val_type_loss: 2.2904 - learning_rate: 3.0000e-04\n",
            "Epoch 2/3\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 30ms/step - ie_acc: 0.7668 - ie_loss: 0.0691 - jp_acc: 0.6043 - jp_loss: 0.0841 - loss: 4.8484 - sn_acc: 0.8627 - sn_loss: 0.0525 - tf_acc: 0.6139 - tf_loss: 0.0821 - type_acc: 0.2391 - type_loss: 2.2803 - val_ie_acc: 0.7678 - val_ie_loss: 0.0689 - val_jp_acc: 0.6069 - val_jp_loss: 0.0837 - val_loss: 4.8513 - val_sn_acc: 0.8659 - val_sn_loss: 0.0518 - val_tf_acc: 0.6066 - val_tf_loss: 0.0825 - val_type_acc: 0.2366 - val_type_loss: 2.2820 - learning_rate: 3.0000e-04\n",
            "Epoch 3/3\n",
            "\u001b[1m1445/1445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - ie_acc: 0.7670 - ie_loss: 0.0677 - jp_acc: 0.6159 - jp_loss: 0.0825 - loss: 4.7435 - sn_acc: 0.8621 - sn_loss: 0.0515 - tf_acc: 0.6406 - tf_loss: 0.0799 - type_acc: 0.2679 - type_loss: 2.2310 - val_ie_acc: 0.7680 - val_ie_loss: 0.0691 - val_jp_acc: 0.6040 - val_jp_loss: 0.0837 - val_loss: 4.8656 - val_sn_acc: 0.8659 - val_sn_loss: 0.0520 - val_tf_acc: 0.6051 - val_tf_loss: 0.0830 - val_type_acc: 0.2361 - val_type_loss: 2.2887 - learning_rate: 3.0000e-04\n",
            "\n",
            "Multi-task type head on Kaggle val:\n",
            "  accuracy: 0.1454819790221703\n",
            "  macro-F1: 0.0315865168037644\n"
          ]
        }
      ]
    }
  ]
}